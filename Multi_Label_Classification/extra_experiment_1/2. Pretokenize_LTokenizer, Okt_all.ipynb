{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_excel('./data/all_10_random_12000_262_train.xlsx')\n",
    "val = pd.read_excel('./data/all_10_random_12000_262_val.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abst = train['abstract']\n",
    "val_abst = val['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_abst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_abst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent = list(train_abst)\n",
    "#train_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sent = list(val_abst)\n",
    "#val_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_sent + val_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_extractor = WordExtractor(min_frequency = 1, min_cohesion_forward = 0.0, min_right_branching_entropy = 0.0)\n",
    "\n",
    "word_extractor.train(sentences)\n",
    "word_extractor.save('./data/embedding/wordextractor_model_all_random_262')\n",
    "\n",
    "words = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = word_extractor.word_scores()\n",
    "cal_scores = {key:(scores[key].cohesion_forward * math.exp(scores[key].right_branching_entropy)) for key in scores.keys()}\n",
    "tokenizer = LTokenizer(scores = cal_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltokenized_data = []\n",
    "\n",
    "\n",
    "f = open('./data/embedding/user_dic_all_random_262.txt', 'w', encoding='UTF8')\n",
    "for sentence in tqdm(sentences):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        token = token+'\\n'\n",
    "        f.write(token)\n",
    "        ltokenized_data.append(token)\n",
    "        print(token)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ltokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "tokenized_data = []\n",
    "\n",
    "\n",
    "for sentence in tqdm(sentences):\n",
    "    temp_X = okt.morphs(sentence)\n",
    "    tokenized_data.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open('./data/embedding/original_tokenized_data_all_random_262.pickle', 'wb') as f:\n",
    "    pickle.dump(tokenized_data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open('./data/embedding/original_tokenized_data_all_random_262.pickle', 'rb') as f:\n",
    "    tokenized_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ltokenize + okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "tokenized_data = []\n",
    "\n",
    "\n",
    "for sentence in tqdm(sentences):\n",
    "    temp_X = okt.morphs(sentence)\n",
    "    tokenized_data.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# okt 사용자 사전 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/user/.conda/envs/mlc2/lib/site-packages/konlpy/java')\n",
    "os.getcwd() \n",
    "os.makedirs('./userdic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jar xvf open-korean-text-2.1.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jar cvf open-korean-text-2.1.0.jar *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "os.chdir('D:/Anaconda3/envs/han/lib/site-packages/konlpy/java')\n",
    "os.getcwd() \n",
    "os.makedirs('./userdic')\n",
    "\n",
    "\n",
    "!jar xvf ../open-korean-text-2.1.0.jar\n",
    "\n",
    "\n",
    "with open(f\"D:/Anaconda3/envs/han/lib/site-packages/konlpy/java/userdic/org/openkoreantext/processor/util/noun/names.txt\", encoding='UTF8') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "    \n",
    "for tokens in tqdm(ltokenized_data):\n",
    "    for token in tokens:\n",
    "        data += token + '\\n'\n",
    "        \n",
    "        \n",
    "with open(\"D:/Anaconda3/envs/han/lib/site-packages/konlpy/java/userdic/org/openkoreantext/processor/util/noun/names.txt\", 'w', encoding='UTF8') as f:\n",
    "    f.write(data)\n",
    "    \n",
    "    \n",
    "!jar cvf ../open-korean-text-2.1.0.jar * \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 명사 추출 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "noun_extractor = LRNounExtractor_v2()\n",
    "nouns = noun_extractor.train_extract(sentences) # list of str like\n",
    "\n",
    "noun_scores = {noun:score.score for noun, score in nouns.items()}\n",
    "cohesion_score = {word:score.cohesion_forward for word, score in words.items()}\n",
    "combined_scores = {noun:score + cohesion_score.get(noun, 0) for noun, score in noun_scores.items()}\n",
    "combined_scores.update({subword:cohesion for subword, cohesion in cohesion_score.items() if not (subword in combined_scores)})\n",
    "\n",
    "tokenizer = LTokenizer(scores=combined_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "noun_tokenized_data = []\n",
    "\n",
    "\n",
    "for sentence in tqdm(sentences):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    noun_tokenized_data.append(tokens)\n",
    "    print(tokens)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
