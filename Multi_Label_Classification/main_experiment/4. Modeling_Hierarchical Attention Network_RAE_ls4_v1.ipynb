{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13496215365455974165\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15960431820725257162\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4985044352\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9109311595547134033\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1660, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 10479665148008384083\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "version='v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel('./data/rae_v17_train.xlsx')\n",
    "val = pd.read_excel('./data/rae_v17_val.xlsx')\n",
    "test = pd.read_excel('./data/rae_v17_test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Modbus는 각종 자동화 장비 감시 및 제어에 전 세계적으로 널리 사용되고 있는 ...</td>\n",
       "      <td>1.237585</td>\n",
       "      <td>2.008186</td>\n",
       "      <td>1.651276</td>\n",
       "      <td>3.128366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>고속철도의 출현과 함께 철도는 국내외에서 자주 사용하는 교통수단 중 하나이다. 또한...</td>\n",
       "      <td>0.535180</td>\n",
       "      <td>0.071971</td>\n",
       "      <td>0.841932</td>\n",
       "      <td>0.685688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>마찰력은 일상생활에서 뉴턴 역학을 이해하는데 매우 중요한 힘임에도불구하고 많은 학생...</td>\n",
       "      <td>11.045432</td>\n",
       "      <td>6.944512</td>\n",
       "      <td>7.842990</td>\n",
       "      <td>7.482370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>현재 진행형인 4차 산업혁명 사회는 기존 사회와는 여러 가지 면에서 구분되며, 그 ...</td>\n",
       "      <td>3.915327</td>\n",
       "      <td>4.413543</td>\n",
       "      <td>3.069937</td>\n",
       "      <td>4.308259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>최근, 급격한 산업화로 인한 급속한 기후변화가 생태 보전 및 생물 다양성에 부정적인...</td>\n",
       "      <td>4.100452</td>\n",
       "      <td>4.639769</td>\n",
       "      <td>7.940576</td>\n",
       "      <td>4.091141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>2800</td>\n",
       "      <td>딥러닝에 기반 한 인공지능과 다양한 센서기술의 발전이 빠르게 진행되면서 운전자 없이...</td>\n",
       "      <td>1.641130</td>\n",
       "      <td>4.729570</td>\n",
       "      <td>6.027492</td>\n",
       "      <td>6.899787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2801</th>\n",
       "      <td>2801</td>\n",
       "      <td>본 연구는 콜버그 도덕·윤리교육의 ``정의 공동체 접근법``을 공학윤리교육에 적용하...</td>\n",
       "      <td>2.322318</td>\n",
       "      <td>0.749738</td>\n",
       "      <td>1.464523</td>\n",
       "      <td>0.995229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2802</th>\n",
       "      <td>2802</td>\n",
       "      <td>인공지능 로봇이 사회적, 정서적 상호작용의 대상으로서 사람들의 일상적 공간 안으로 ...</td>\n",
       "      <td>0.933682</td>\n",
       "      <td>1.643395</td>\n",
       "      <td>1.009095</td>\n",
       "      <td>1.121973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>2803</td>\n",
       "      <td>인공지능 로봇과 연관된 기술의 발전 속도가 빨라짐에 따라 기존에 인간에 의해서만 이...</td>\n",
       "      <td>1.701093</td>\n",
       "      <td>3.295210</td>\n",
       "      <td>1.718543</td>\n",
       "      <td>4.758762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804</th>\n",
       "      <td>2804</td>\n",
       "      <td>현재 전세계 배터리 시장은 이차전지 개발에 박차를 가하고 있는 실정이지만, 실제로 ...</td>\n",
       "      <td>5.659124</td>\n",
       "      <td>3.762170</td>\n",
       "      <td>6.620386</td>\n",
       "      <td>0.695413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2805 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                           abstract  \\\n",
       "0              0  Modbus는 각종 자동화 장비 감시 및 제어에 전 세계적으로 널리 사용되고 있는 ...   \n",
       "1              1  고속철도의 출현과 함께 철도는 국내외에서 자주 사용하는 교통수단 중 하나이다. 또한...   \n",
       "2              2  마찰력은 일상생활에서 뉴턴 역학을 이해하는데 매우 중요한 힘임에도불구하고 많은 학생...   \n",
       "3              3  현재 진행형인 4차 산업혁명 사회는 기존 사회와는 여러 가지 면에서 구분되며, 그 ...   \n",
       "4              4  최근, 급격한 산업화로 인한 급속한 기후변화가 생태 보전 및 생물 다양성에 부정적인...   \n",
       "...          ...                                                ...   \n",
       "2800        2800  딥러닝에 기반 한 인공지능과 다양한 센서기술의 발전이 빠르게 진행되면서 운전자 없이...   \n",
       "2801        2801  본 연구는 콜버그 도덕·윤리교육의 ``정의 공동체 접근법``을 공학윤리교육에 적용하...   \n",
       "2802        2802  인공지능 로봇이 사회적, 정서적 상호작용의 대상으로서 사람들의 일상적 공간 안으로 ...   \n",
       "2803        2803  인공지능 로봇과 연관된 기술의 발전 속도가 빨라짐에 따라 기존에 인간에 의해서만 이...   \n",
       "2804        2804  현재 전세계 배터리 시장은 이차전지 개발에 박차를 가하고 있는 실정이지만, 실제로 ...   \n",
       "\n",
       "              0         1         2         3  \n",
       "0      1.237585  2.008186  1.651276  3.128366  \n",
       "1      0.535180  0.071971  0.841932  0.685688  \n",
       "2     11.045432  6.944512  7.842990  7.482370  \n",
       "3      3.915327  4.413543  3.069937  4.308259  \n",
       "4      4.100452  4.639769  7.940576  4.091141  \n",
       "...         ...       ...       ...       ...  \n",
       "2800   1.641130  4.729570  6.027492  6.899787  \n",
       "2801   2.322318  0.749738  1.464523  0.995229  \n",
       "2802   0.933682  1.643395  1.009095  1.121973  \n",
       "2803   1.701093  3.295210  1.718543  4.758762  \n",
       "2804   5.659124  3.762170  6.620386  0.695413  \n",
       "\n",
       "[2805 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>로지스틱 회귀분석은 통계학 등의 분야에서 예측을 위한 기술 혹은 변수 간의 상관관계...</td>\n",
       "      <td>2.311226</td>\n",
       "      <td>1.456479</td>\n",
       "      <td>2.298644</td>\n",
       "      <td>1.843304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>최근에 이르러, 기계학습 및 데이터마이닝은 수많은 질병 예측 및 진단에 활용되고 있...</td>\n",
       "      <td>1.646191</td>\n",
       "      <td>1.996787</td>\n",
       "      <td>4.596471</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>가상발전소 시장에 전력을 안정적으로 공급하기 위해서는 발전량에 대한 정확한 예측이 ...</td>\n",
       "      <td>0.117179</td>\n",
       "      <td>0.159195</td>\n",
       "      <td>1.582059</td>\n",
       "      <td>0.915652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>문서 자동 요약은 주어진 문서로부터 주요 내용을 추출하거나 생성하는 방식으로 축약하...</td>\n",
       "      <td>0.117909</td>\n",
       "      <td>0.208360</td>\n",
       "      <td>1.340475</td>\n",
       "      <td>0.683269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>최근 소프트웨어와 인공지능 교육이 점차 중요하게 다루어지면서 2019년 12월 과학...</td>\n",
       "      <td>4.569798</td>\n",
       "      <td>3.530068</td>\n",
       "      <td>2.915969</td>\n",
       "      <td>5.492957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>930</td>\n",
       "      <td>인간의 특성과 관련된 측정 항목을 나타내는 생체정보는 도난이나 분실의 염려가 없으므...</td>\n",
       "      <td>1.228936</td>\n",
       "      <td>0.737807</td>\n",
       "      <td>1.940096</td>\n",
       "      <td>4.251618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>931</td>\n",
       "      <td>제4차 산업혁명이 도래하면서 인공지능 로봇은 다양한 영화 및 드라마에서 다른 캐릭터...</td>\n",
       "      <td>1.701094</td>\n",
       "      <td>3.295210</td>\n",
       "      <td>1.718543</td>\n",
       "      <td>4.758762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>932</td>\n",
       "      <td>초등학교 소프트웨어 교육에서는 간단한 문제해결 과정을 통하여 프로그래밍 과정을 경험...</td>\n",
       "      <td>0.793680</td>\n",
       "      <td>1.689059</td>\n",
       "      <td>0.229706</td>\n",
       "      <td>0.519041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>933</td>\n",
       "      <td>사물인터넷(IoT) 제품 등의 보안이 허술해 각종 해킹 사고가 발생하고 있다. 보안...</td>\n",
       "      <td>0.325306</td>\n",
       "      <td>3.757606</td>\n",
       "      <td>5.960067</td>\n",
       "      <td>0.750967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>934</td>\n",
       "      <td>이 글은 북한의 4차 산업혁명에 대한 대응전략과 추진방식, 그리고 일부 성과들을  ...</td>\n",
       "      <td>3.915327</td>\n",
       "      <td>4.413543</td>\n",
       "      <td>3.069937</td>\n",
       "      <td>4.308259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>935 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                           abstract         0  \\\n",
       "0             0  로지스틱 회귀분석은 통계학 등의 분야에서 예측을 위한 기술 혹은 변수 간의 상관관계...  2.311226   \n",
       "1             1  최근에 이르러, 기계학습 및 데이터마이닝은 수많은 질병 예측 및 진단에 활용되고 있...  1.646191   \n",
       "2             2  가상발전소 시장에 전력을 안정적으로 공급하기 위해서는 발전량에 대한 정확한 예측이 ...  0.117179   \n",
       "3             3  문서 자동 요약은 주어진 문서로부터 주요 내용을 추출하거나 생성하는 방식으로 축약하...  0.117909   \n",
       "4             4  최근 소프트웨어와 인공지능 교육이 점차 중요하게 다루어지면서 2019년 12월 과학...  4.569798   \n",
       "..          ...                                                ...       ...   \n",
       "930         930  인간의 특성과 관련된 측정 항목을 나타내는 생체정보는 도난이나 분실의 염려가 없으므...  1.228936   \n",
       "931         931  제4차 산업혁명이 도래하면서 인공지능 로봇은 다양한 영화 및 드라마에서 다른 캐릭터...  1.701094   \n",
       "932         932  초등학교 소프트웨어 교육에서는 간단한 문제해결 과정을 통하여 프로그래밍 과정을 경험...  0.793680   \n",
       "933         933  사물인터넷(IoT) 제품 등의 보안이 허술해 각종 해킹 사고가 발생하고 있다. 보안...  0.325306   \n",
       "934         934  이 글은 북한의 4차 산업혁명에 대한 대응전략과 추진방식, 그리고 일부 성과들을  ...  3.915327   \n",
       "\n",
       "            1         2         3  \n",
       "0    1.456479  2.298644  1.843304  \n",
       "1    1.996787  4.596471  0.000000  \n",
       "2    0.159195  1.582059  0.915652  \n",
       "3    0.208360  1.340475  0.683269  \n",
       "4    3.530068  2.915969  5.492957  \n",
       "..        ...       ...       ...  \n",
       "930  0.737807  1.940096  4.251618  \n",
       "931  3.295210  1.718543  4.758762  \n",
       "932  1.689059  0.229706  0.519041  \n",
       "933  3.757606  5.960067  0.750967  \n",
       "934  4.413543  3.069937  4.308259  \n",
       "\n",
       "[935 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>abstract</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>인류는 1800년대 1차 산업혁명을 거치면서 급격하게 변화하고 성장하였다. 약 20...</td>\n",
       "      <td>2.704093</td>\n",
       "      <td>8.809292</td>\n",
       "      <td>9.884574</td>\n",
       "      <td>7.197733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>본 연구는 Deep Neural Network(DNN)을 이용하여 광주-기아 챔피언...</td>\n",
       "      <td>6.213107</td>\n",
       "      <td>2.623662</td>\n",
       "      <td>4.164218</td>\n",
       "      <td>0.464886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>오늘날 기업이 급변하는 글로벌 경영 환경에 민첩하게 대응하기 위해서는 기업 구성원 ...</td>\n",
       "      <td>2.734857</td>\n",
       "      <td>1.507144</td>\n",
       "      <td>1.873411</td>\n",
       "      <td>2.756508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>도시에서 홍수 피해를 방지하기 위한 침수를 예측하기 위해 본 논문에서는 딥러닝(De...</td>\n",
       "      <td>1.228936</td>\n",
       "      <td>0.737808</td>\n",
       "      <td>1.940097</td>\n",
       "      <td>4.251619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>컴퓨터가 실생활에서 많이 사용됨에 따라, 악성코드(malware)를 만들어 악의적인...</td>\n",
       "      <td>1.032983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.069249</td>\n",
       "      <td>1.546211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>930</td>\n",
       "      <td>포스트 휴머니즘의 한 갈래로서 트랜스 휴머니즘은 ‘의지적 진화’에 의한 인간의 개선...</td>\n",
       "      <td>3.642006</td>\n",
       "      <td>2.489532</td>\n",
       "      <td>0.381377</td>\n",
       "      <td>2.429291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>931</td>\n",
       "      <td>4차 산업혁명 진전되는 인공지능 시대 노동법제 논의가 필요한 과제는 다음과 같다.첫...</td>\n",
       "      <td>6.362803</td>\n",
       "      <td>7.149104</td>\n",
       "      <td>6.364623</td>\n",
       "      <td>7.359859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>932</td>\n",
       "      <td>본 연구는 현대 패션쇼에 나타난 포스트휴먼의 내적 특성에 관한 연구이다. 연구의 목...</td>\n",
       "      <td>4.961435</td>\n",
       "      <td>4.227638</td>\n",
       "      <td>1.423454</td>\n",
       "      <td>3.006711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>933</td>\n",
       "      <td>제4차 산업혁명이라는 유행어가 잘 보여주는 것처럼 현대 과학기술은 사회구조 및 사고...</td>\n",
       "      <td>2.194763</td>\n",
       "      <td>1.988368</td>\n",
       "      <td>1.496768</td>\n",
       "      <td>1.338050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>934</td>\n",
       "      <td>(연구배경 및 목적) 생활 방식 변화로 1인 가구의 증가율은 전 연령대에서 지속해서...</td>\n",
       "      <td>1.606113</td>\n",
       "      <td>1.506058</td>\n",
       "      <td>2.320512</td>\n",
       "      <td>0.952771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>935 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                           abstract         0  \\\n",
       "0             0  인류는 1800년대 1차 산업혁명을 거치면서 급격하게 변화하고 성장하였다. 약 20...  2.704093   \n",
       "1             1  본 연구는 Deep Neural Network(DNN)을 이용하여 광주-기아 챔피언...  6.213107   \n",
       "2             2  오늘날 기업이 급변하는 글로벌 경영 환경에 민첩하게 대응하기 위해서는 기업 구성원 ...  2.734857   \n",
       "3             3  도시에서 홍수 피해를 방지하기 위한 침수를 예측하기 위해 본 논문에서는 딥러닝(De...  1.228936   \n",
       "4             4  컴퓨터가 실생활에서 많이 사용됨에 따라, 악성코드(malware)를 만들어 악의적인...  1.032983   \n",
       "..          ...                                                ...       ...   \n",
       "930         930  포스트 휴머니즘의 한 갈래로서 트랜스 휴머니즘은 ‘의지적 진화’에 의한 인간의 개선...  3.642006   \n",
       "931         931  4차 산업혁명 진전되는 인공지능 시대 노동법제 논의가 필요한 과제는 다음과 같다.첫...  6.362803   \n",
       "932         932  본 연구는 현대 패션쇼에 나타난 포스트휴먼의 내적 특성에 관한 연구이다. 연구의 목...  4.961435   \n",
       "933         933  제4차 산업혁명이라는 유행어가 잘 보여주는 것처럼 현대 과학기술은 사회구조 및 사고...  2.194763   \n",
       "934         934  (연구배경 및 목적) 생활 방식 변화로 1인 가구의 증가율은 전 연령대에서 지속해서...  1.606113   \n",
       "\n",
       "            1         2         3  \n",
       "0    8.809292  9.884574  7.197733  \n",
       "1    2.623662  4.164218  0.464886  \n",
       "2    1.507144  1.873411  2.756508  \n",
       "3    0.737808  1.940097  4.251619  \n",
       "4    0.000000  2.069249  1.546211  \n",
       "..        ...       ...       ...  \n",
       "930  2.489532  0.381377  2.429291  \n",
       "931  7.149104  6.364623  7.359859  \n",
       "932  4.227638  1.423454  3.006711  \n",
       "933  1.988368  1.496768  1.338050  \n",
       "934  1.506058  2.320512  0.952771  \n",
       "\n",
       "[935 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Modbus는 각종 자동화 장비 감시 및 제어에 전 세계적으로 널리 사용되고 있는 ...\n",
       "1       고속철도의 출현과 함께 철도는 국내외에서 자주 사용하는 교통수단 중 하나이다. 또한...\n",
       "2       마찰력은 일상생활에서 뉴턴 역학을 이해하는데 매우 중요한 힘임에도불구하고 많은 학생...\n",
       "3       현재 진행형인 4차 산업혁명 사회는 기존 사회와는 여러 가지 면에서 구분되며, 그 ...\n",
       "4       최근, 급격한 산업화로 인한 급속한 기후변화가 생태 보전 및 생물 다양성에 부정적인...\n",
       "                              ...                        \n",
       "2800    딥러닝에 기반 한 인공지능과 다양한 센서기술의 발전이 빠르게 진행되면서 운전자 없이...\n",
       "2801    본 연구는 콜버그 도덕·윤리교육의 ``정의 공동체 접근법``을 공학윤리교육에 적용하...\n",
       "2802    인공지능 로봇이 사회적, 정서적 상호작용의 대상으로서 사람들의 일상적 공간 안으로 ...\n",
       "2803    인공지능 로봇과 연관된 기술의 발전 속도가 빨라짐에 따라 기존에 인간에 의해서만 이...\n",
       "2804    현재 전세계 배터리 시장은 이차전지 개발에 박차를 가하고 있는 실정이지만, 실제로 ...\n",
       "Name: abstract, Length: 2805, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = train['abstract']\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.237585</td>\n",
       "      <td>2.008186</td>\n",
       "      <td>1.651276</td>\n",
       "      <td>3.128366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.535180</td>\n",
       "      <td>0.071971</td>\n",
       "      <td>0.841932</td>\n",
       "      <td>0.685688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.045432</td>\n",
       "      <td>6.944512</td>\n",
       "      <td>7.842990</td>\n",
       "      <td>7.482370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.915327</td>\n",
       "      <td>4.413543</td>\n",
       "      <td>3.069937</td>\n",
       "      <td>4.308259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.100452</td>\n",
       "      <td>4.639769</td>\n",
       "      <td>7.940576</td>\n",
       "      <td>4.091141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>1.641130</td>\n",
       "      <td>4.729570</td>\n",
       "      <td>6.027492</td>\n",
       "      <td>6.899787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2801</th>\n",
       "      <td>2.322318</td>\n",
       "      <td>0.749738</td>\n",
       "      <td>1.464523</td>\n",
       "      <td>0.995229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2802</th>\n",
       "      <td>0.933682</td>\n",
       "      <td>1.643395</td>\n",
       "      <td>1.009095</td>\n",
       "      <td>1.121973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>1.701093</td>\n",
       "      <td>3.295210</td>\n",
       "      <td>1.718543</td>\n",
       "      <td>4.758762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804</th>\n",
       "      <td>5.659124</td>\n",
       "      <td>3.762170</td>\n",
       "      <td>6.620386</td>\n",
       "      <td>0.695413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2805 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3\n",
       "0      1.237585  2.008186  1.651276  3.128366\n",
       "1      0.535180  0.071971  0.841932  0.685688\n",
       "2     11.045432  6.944512  7.842990  7.482370\n",
       "3      3.915327  4.413543  3.069937  4.308259\n",
       "4      4.100452  4.639769  7.940576  4.091141\n",
       "...         ...       ...       ...       ...\n",
       "2800   1.641130  4.729570  6.027492  6.899787\n",
       "2801   2.322318  0.749738  1.464523  0.995229\n",
       "2802   0.933682  1.643395  1.009095  1.121973\n",
       "2803   1.701093  3.295210  1.718543  4.758762\n",
       "2804   5.659124  3.762170  6.620386  0.695413\n",
       "\n",
       "[2805 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = train.drop(['Unnamed: 0', 'abstract'], axis=1)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      로지스틱 회귀분석은 통계학 등의 분야에서 예측을 위한 기술 혹은 변수 간의 상관관계...\n",
       "1      최근에 이르러, 기계학습 및 데이터마이닝은 수많은 질병 예측 및 진단에 활용되고 있...\n",
       "2      가상발전소 시장에 전력을 안정적으로 공급하기 위해서는 발전량에 대한 정확한 예측이 ...\n",
       "3      문서 자동 요약은 주어진 문서로부터 주요 내용을 추출하거나 생성하는 방식으로 축약하...\n",
       "4      최근 소프트웨어와 인공지능 교육이 점차 중요하게 다루어지면서 2019년 12월 과학...\n",
       "                             ...                        \n",
       "930    인간의 특성과 관련된 측정 항목을 나타내는 생체정보는 도난이나 분실의 염려가 없으므...\n",
       "931    제4차 산업혁명이 도래하면서 인공지능 로봇은 다양한 영화 및 드라마에서 다른 캐릭터...\n",
       "932    초등학교 소프트웨어 교육에서는 간단한 문제해결 과정을 통하여 프로그래밍 과정을 경험...\n",
       "933    사물인터넷(IoT) 제품 등의 보안이 허술해 각종 해킹 사고가 발생하고 있다. 보안...\n",
       "934    이 글은 북한의 4차 산업혁명에 대한 대응전략과 추진방식, 그리고 일부 성과들을  ...\n",
       "Name: abstract, Length: 935, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_X = val['abstract']\n",
    "val_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.311226</td>\n",
       "      <td>1.456479</td>\n",
       "      <td>2.298644</td>\n",
       "      <td>1.843304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.646191</td>\n",
       "      <td>1.996787</td>\n",
       "      <td>4.596471</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117179</td>\n",
       "      <td>0.159195</td>\n",
       "      <td>1.582059</td>\n",
       "      <td>0.915652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.117909</td>\n",
       "      <td>0.208360</td>\n",
       "      <td>1.340475</td>\n",
       "      <td>0.683269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.569798</td>\n",
       "      <td>3.530068</td>\n",
       "      <td>2.915969</td>\n",
       "      <td>5.492957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>1.228936</td>\n",
       "      <td>0.737807</td>\n",
       "      <td>1.940096</td>\n",
       "      <td>4.251618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>1.701094</td>\n",
       "      <td>3.295210</td>\n",
       "      <td>1.718543</td>\n",
       "      <td>4.758762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>0.793680</td>\n",
       "      <td>1.689059</td>\n",
       "      <td>0.229706</td>\n",
       "      <td>0.519041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>0.325306</td>\n",
       "      <td>3.757606</td>\n",
       "      <td>5.960067</td>\n",
       "      <td>0.750967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>3.915327</td>\n",
       "      <td>4.413543</td>\n",
       "      <td>3.069937</td>\n",
       "      <td>4.308259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>935 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3\n",
       "0    2.311226  1.456479  2.298644  1.843304\n",
       "1    1.646191  1.996787  4.596471  0.000000\n",
       "2    0.117179  0.159195  1.582059  0.915652\n",
       "3    0.117909  0.208360  1.340475  0.683269\n",
       "4    4.569798  3.530068  2.915969  5.492957\n",
       "..        ...       ...       ...       ...\n",
       "930  1.228936  0.737807  1.940096  4.251618\n",
       "931  1.701094  3.295210  1.718543  4.758762\n",
       "932  0.793680  1.689059  0.229706  0.519041\n",
       "933  0.325306  3.757606  5.960067  0.750967\n",
       "934  3.915327  4.413543  3.069937  4.308259\n",
       "\n",
       "[935 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y = val.drop(['Unnamed: 0', 'abstract'], axis=1)\n",
    "val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      인류는 1800년대 1차 산업혁명을 거치면서 급격하게 변화하고 성장하였다. 약 20...\n",
       "1      본 연구는 Deep Neural Network(DNN)을 이용하여 광주-기아 챔피언...\n",
       "2      오늘날 기업이 급변하는 글로벌 경영 환경에 민첩하게 대응하기 위해서는 기업 구성원 ...\n",
       "3      도시에서 홍수 피해를 방지하기 위한 침수를 예측하기 위해 본 논문에서는 딥러닝(De...\n",
       "4      컴퓨터가 실생활에서 많이 사용됨에 따라, 악성코드(malware)를 만들어 악의적인...\n",
       "                             ...                        \n",
       "930    포스트 휴머니즘의 한 갈래로서 트랜스 휴머니즘은 ‘의지적 진화’에 의한 인간의 개선...\n",
       "931    4차 산업혁명 진전되는 인공지능 시대 노동법제 논의가 필요한 과제는 다음과 같다.첫...\n",
       "932    본 연구는 현대 패션쇼에 나타난 포스트휴먼의 내적 특성에 관한 연구이다. 연구의 목...\n",
       "933    제4차 산업혁명이라는 유행어가 잘 보여주는 것처럼 현대 과학기술은 사회구조 및 사고...\n",
       "934    (연구배경 및 목적) 생활 방식 변화로 1인 가구의 증가율은 전 연령대에서 지속해서...\n",
       "Name: abstract, Length: 935, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = test['abstract']\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.704093</td>\n",
       "      <td>8.809292</td>\n",
       "      <td>9.884574</td>\n",
       "      <td>7.197733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.213107</td>\n",
       "      <td>2.623662</td>\n",
       "      <td>4.164218</td>\n",
       "      <td>0.464886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.734857</td>\n",
       "      <td>1.507144</td>\n",
       "      <td>1.873411</td>\n",
       "      <td>2.756508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.228936</td>\n",
       "      <td>0.737808</td>\n",
       "      <td>1.940097</td>\n",
       "      <td>4.251619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.032983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.069249</td>\n",
       "      <td>1.546211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>3.642006</td>\n",
       "      <td>2.489532</td>\n",
       "      <td>0.381377</td>\n",
       "      <td>2.429291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>6.362803</td>\n",
       "      <td>7.149104</td>\n",
       "      <td>6.364623</td>\n",
       "      <td>7.359859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>4.961435</td>\n",
       "      <td>4.227638</td>\n",
       "      <td>1.423454</td>\n",
       "      <td>3.006711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>2.194763</td>\n",
       "      <td>1.988368</td>\n",
       "      <td>1.496768</td>\n",
       "      <td>1.338050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>1.606113</td>\n",
       "      <td>1.506058</td>\n",
       "      <td>2.320512</td>\n",
       "      <td>0.952771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>935 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3\n",
       "0    2.704093  8.809292  9.884574  7.197733\n",
       "1    6.213107  2.623662  4.164218  0.464886\n",
       "2    2.734857  1.507144  1.873411  2.756508\n",
       "3    1.228936  0.737808  1.940097  4.251619\n",
       "4    1.032983  0.000000  2.069249  1.546211\n",
       "..        ...       ...       ...       ...\n",
       "930  3.642006  2.489532  0.381377  2.429291\n",
       "931  6.362803  7.149104  6.364623  7.359859\n",
       "932  4.961435  4.227638  1.423454  3.006711\n",
       "933  2.194763  1.988368  1.496768  1.338050\n",
       "934  1.606113  1.506058  2.320512  0.952771\n",
       "\n",
       "[935 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = test.drop(['Unnamed: 0', 'abstract'], axis=1)\n",
    "test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import re\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text_list):\n",
    "    \n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣0-9]+')\n",
    "    stopwords = ['을', '를', '이', '가', '은', '는', '의']\n",
    "    tokenizer = Okt() #형태소 분석기 \n",
    "    token_list = []\n",
    "    \n",
    "    for text in tqdm(text_list):\n",
    "        txt = hangul.sub('', text)\n",
    "        token = tokenizer.morphs(txt)\n",
    "        token = [t for t in token if t not in stopwords or type(t) != float]\n",
    "        token_list.append(token)\n",
    "        \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2805/2805 [01:15<00:00, 37.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 935/935 [00:27<00:00, 34.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 935/935 [00:29<00:00, 32.17it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sent_token = text_preprocessing(train_X)\n",
    "val_sent_token = text_preprocessing(val_X)\n",
    "test_sent_token = text_preprocessing(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['로지',\n",
       "  '스틱',\n",
       "  '회귀분석은',\n",
       "  '통계학',\n",
       "  '등의',\n",
       "  '분야에서',\n",
       "  '예측을',\n",
       "  '위한',\n",
       "  '기술',\n",
       "  '혹은',\n",
       "  '변수',\n",
       "  '간의',\n",
       "  '상관관계',\n",
       "  '를',\n",
       "  '설명',\n",
       "  '하기',\n",
       "  '위하여',\n",
       "  '오랫동안',\n",
       "  '사용',\n",
       "  '되어',\n",
       "  '왔다',\n",
       "  '이러한',\n",
       "  '로지',\n",
       "  '스틱',\n",
       "  '회귀분석',\n",
       "  '방법에서',\n",
       "  '현재',\n",
       "  '각',\n",
       "  '속성',\n",
       "  '들',\n",
       "  '은',\n",
       "  '목적',\n",
       "  '값에',\n",
       "  '대하여',\n",
       "  '동일한',\n",
       "  '중요도를',\n",
       "  '가지고',\n",
       "  '있다',\n",
       "  '본',\n",
       "  '연구에서는',\n",
       "  '이러한',\n",
       "  '가중치',\n",
       "  '계산',\n",
       "  '을',\n",
       "  '좀더',\n",
       "  '세분화',\n",
       "  '하여',\n",
       "  '각',\n",
       "  '속성',\n",
       "  '의',\n",
       "  '값이',\n",
       "  '서로',\n",
       "  '다른',\n",
       "  '중요도를',\n",
       "  '가지는',\n",
       "  '새로운',\n",
       "  '학습',\n",
       "  '방법을',\n",
       "  '제시',\n",
       "  '한다',\n",
       "  '알고리즘의',\n",
       "  '성능을',\n",
       "  '최대',\n",
       "  '화하는',\n",
       "  '각',\n",
       "  '속성',\n",
       "  '값',\n",
       "  '가중치',\n",
       "  '의',\n",
       "  '값을',\n",
       "  '계산',\n",
       "  '하기',\n",
       "  '위하여',\n",
       "  '점진적',\n",
       "  '하강법을',\n",
       "  '이용하여',\n",
       "  '개발',\n",
       "  '하였다',\n",
       "  '본',\n",
       "  '연구에서',\n",
       "  '제안된',\n",
       "  '방법은',\n",
       "  '다양한',\n",
       "  '데이터를',\n",
       "  '이용하여',\n",
       "  '실험',\n",
       "  '하였고',\n",
       "  '속성',\n",
       "  '값',\n",
       "  '기반',\n",
       "  '로지',\n",
       "  '스틱',\n",
       "  '회귀분석',\n",
       "  '방법은',\n",
       "  '기존의',\n",
       "  '로지',\n",
       "  '스틱',\n",
       "  '회귀분석',\n",
       "  '보다',\n",
       "  '우수한',\n",
       "  '학습',\n",
       "  '능력을',\n",
       "  '보임을',\n",
       "  '알',\n",
       "  '수',\n",
       "  '있었다'],\n",
       " ['최근에',\n",
       "  '이르러',\n",
       "  '기계학습',\n",
       "  '및',\n",
       "  '데이터마이닝',\n",
       "  '은',\n",
       "  '수많은',\n",
       "  '질병',\n",
       "  '예측',\n",
       "  '및',\n",
       "  '진단에',\n",
       "  '활용되고',\n",
       "  '있다',\n",
       "  '만성질환',\n",
       "  '은',\n",
       "  '전체',\n",
       "  '사망률',\n",
       "  '의',\n",
       "  '약',\n",
       "  '80',\n",
       "  '를',\n",
       "  '차지하는',\n",
       "  '질병',\n",
       "  '으로',\n",
       "  '점점',\n",
       "  '증가',\n",
       "  '하는',\n",
       "  '추세',\n",
       "  '이다',\n",
       "  '만성질환',\n",
       "  '관련',\n",
       "  '예측',\n",
       "  '모델을',\n",
       "  '연구',\n",
       "  '한',\n",
       "  '기존',\n",
       "  '연구들은',\n",
       "  '예측',\n",
       "  '모델을',\n",
       "  '구성하는',\n",
       "  '데이터로',\n",
       "  '혈당',\n",
       "  '혈압',\n",
       "  '인슐린',\n",
       "  '수치',\n",
       "  '등의',\n",
       "  '건강검진',\n",
       "  '수준의',\n",
       "  '데이터를',\n",
       "  '이용',\n",
       "  '한다',\n",
       "  '본',\n",
       "  '논문은',\n",
       "  '만성질환',\n",
       "  '의',\n",
       "  '위험',\n",
       "  '요인인',\n",
       "  '이상',\n",
       "  '지질혈증과',\n",
       "  '안면',\n",
       "  '정보의',\n",
       "  '연관성을',\n",
       "  '검증',\n",
       "  '하고',\n",
       "  '기계학습',\n",
       "  '기반',\n",
       "  '안면',\n",
       "  '정보를',\n",
       "  '이용한',\n",
       "  '이상',\n",
       "  '지질혈증',\n",
       "  '예측',\n",
       "  '모델을',\n",
       "  '세계',\n",
       "  '최초로',\n",
       "  '개발',\n",
       "  '한다',\n",
       "  '본',\n",
       "  '연구는',\n",
       "  '5390',\n",
       "  '명의',\n",
       "  '임상',\n",
       "  '데이터',\n",
       "  '중',\n",
       "  '안면',\n",
       "  '정보와',\n",
       "  '중성지방혈증',\n",
       "  '정보를',\n",
       "  '바탕으로',\n",
       "  '수행',\n",
       "  '하였다',\n",
       "  '중성지방혈증은',\n",
       "  '이상',\n",
       "  '지질혈증을',\n",
       "  '판단하는',\n",
       "  '척도',\n",
       "  '이다',\n",
       "  '연구의',\n",
       "  '결과로',\n",
       "  '얼굴',\n",
       "  '의',\n",
       "  '하악',\n",
       "  '간의',\n",
       "  '거리를',\n",
       "  '나타내는',\n",
       "  '4314300001',\n",
       "  '0652',\n",
       "  '와',\n",
       "  '고중성지방혈증이',\n",
       "  '매우',\n",
       "  '높은',\n",
       "  '연관성을',\n",
       "  '가진',\n",
       "  '것을',\n",
       "  '밝혀냈고',\n",
       "  '이를',\n",
       "  '기반으로',\n",
       "  '구축',\n",
       "  '한',\n",
       "  '모델은',\n",
       "  '0662',\n",
       "  '의',\n",
       "  '값을',\n",
       "  '획득',\n",
       "  '하였다',\n",
       "  '이러한',\n",
       "  '연구결과는',\n",
       "  '향후',\n",
       "  '질병',\n",
       "  '역학',\n",
       "  '및',\n",
       "  '대중',\n",
       "  '보건',\n",
       "  '영역의',\n",
       "  '스크',\n",
       "  '리닝',\n",
       "  '단계에서',\n",
       "  '안면정보만으로',\n",
       "  '다양할',\n",
       "  '질병',\n",
       "  '을',\n",
       "  '예측할',\n",
       "  '수',\n",
       "  '있는',\n",
       "  '기반을',\n",
       "  '제',\n",
       "  '공할',\n",
       "  '수',\n",
       "  '있을',\n",
       "  '것이',\n",
       "  '다'],\n",
       " ['가상',\n",
       "  '발전소',\n",
       "  '시장에',\n",
       "  '전력을',\n",
       "  '안정적으로',\n",
       "  '공급',\n",
       "  '하기',\n",
       "  '위해서는',\n",
       "  '발전',\n",
       "  '량에',\n",
       "  '대한',\n",
       "  '정확한',\n",
       "  '예측이',\n",
       "  '필요하다',\n",
       "  '하지만',\n",
       "  '태양광',\n",
       "  '발전',\n",
       "  '량은',\n",
       "  '기상',\n",
       "  '환경에',\n",
       "  '영향을',\n",
       "  '받아',\n",
       "  '발전',\n",
       "  '량의',\n",
       "  '편차가',\n",
       "  '심하기',\n",
       "  '때문에',\n",
       "  '안정적',\n",
       "  '인',\n",
       "  '예측이',\n",
       "  '어렵다',\n",
       "  '본',\n",
       "  '논문에서는',\n",
       "  '기반',\n",
       "  '태양광',\n",
       "  '발전',\n",
       "  '량',\n",
       "  '예측모델을',\n",
       "  '제안',\n",
       "  '한다',\n",
       "  '우리는',\n",
       "  '기상',\n",
       "  '데이터와',\n",
       "  '기상',\n",
       "  '예보',\n",
       "  '데이터를',\n",
       "  '발전',\n",
       "  '량',\n",
       "  '예측모델에',\n",
       "  '입력',\n",
       "  '하여',\n",
       "  '성능을',\n",
       "  '향상',\n",
       "  '시킨다',\n",
       "  '또한',\n",
       "  '상관계수',\n",
       "  '를',\n",
       "  '기준으로',\n",
       "  '우선순위',\n",
       "  '를',\n",
       "  '설정하',\n",
       "  '여',\n",
       "  '변수를',\n",
       "  '선택',\n",
       "  '한다',\n",
       "  '태양광',\n",
       "  '발전',\n",
       "  '량의',\n",
       "  '상관관계',\n",
       "  '와',\n",
       "  '발전',\n",
       "  '량',\n",
       "  '예측',\n",
       "  '결과를',\n",
       "  '비교하여',\n",
       "  '태양광',\n",
       "  '에너지',\n",
       "  '발전',\n",
       "  '량',\n",
       "  '예측에',\n",
       "  '활용',\n",
       "  '되는',\n",
       "  '최적의',\n",
       "  '기상',\n",
       "  '요인을',\n",
       "  '검토',\n",
       "  '한다'],\n",
       " ['문서',\n",
       "  '자동',\n",
       "  '요약',\n",
       "  '은',\n",
       "  '주어진',\n",
       "  '문서로부터',\n",
       "  '주요',\n",
       "  '내용을',\n",
       "  '추출',\n",
       "  '하거나',\n",
       "  '생성',\n",
       "  '하는',\n",
       "  '방식으로',\n",
       "  '축약',\n",
       "  '하는',\n",
       "  '작업을',\n",
       "  '말',\n",
       "  '한다',\n",
       "  '최근',\n",
       "  '연구에서는',\n",
       "  '대량의',\n",
       "  '문서를',\n",
       "  '딥러닝',\n",
       "  '기법을',\n",
       "  '적용하여',\n",
       "  '요약',\n",
       "  '문',\n",
       "  '자체',\n",
       "  '를',\n",
       "  '생성',\n",
       "  '하는',\n",
       "  '방식으로',\n",
       "  '발전',\n",
       "  '하고',\n",
       "  '있다',\n",
       "  '생성',\n",
       "  '요약',\n",
       "  '은',\n",
       "  '미리',\n",
       "  '생성',\n",
       "  '된',\n",
       "  '위드',\n",
       "  '임베딩',\n",
       "  '정보를',\n",
       "  '사용하는데',\n",
       "  '전문',\n",
       "  '용어와',\n",
       "  '같이',\n",
       "  '저빈도',\n",
       "  '핵심',\n",
       "  '어휘',\n",
       "  '는',\n",
       "  '입베딩',\n",
       "  '된',\n",
       "  '사전',\n",
       "  '에',\n",
       "  '없는',\n",
       "  '문제가',\n",
       "  '발생',\n",
       "  '한다',\n",
       "  '인코딩',\n",
       "  '디코딩',\n",
       "  '신경망',\n",
       "  '모델의',\n",
       "  '문서',\n",
       "  '자동',\n",
       "  '요약',\n",
       "  '에서',\n",
       "  '미등록',\n",
       "  '어휘',\n",
       "  '의',\n",
       "  '출현',\n",
       "  '은',\n",
       "  '요약',\n",
       "  '성능',\n",
       "  '저하',\n",
       "  '의',\n",
       "  '요인',\n",
       "  '이다',\n",
       "  '이를',\n",
       "  '해결',\n",
       "  '하기',\n",
       "  '위해',\n",
       "  '본',\n",
       "  '논문에서는',\n",
       "  '요약',\n",
       "  '대상',\n",
       "  '문서',\n",
       "  '에서',\n",
       "  '새로',\n",
       "  '출현',\n",
       "  '한',\n",
       "  '단어',\n",
       "  '를',\n",
       "  '복사',\n",
       "  '하여',\n",
       "  '요약',\n",
       "  '문을',\n",
       "  '생성',\n",
       "  '하는',\n",
       "  '방법을',\n",
       "  '사용',\n",
       "  '한다',\n",
       "  '기존의',\n",
       "  '연구',\n",
       "  '와는',\n",
       "  '달리',\n",
       "  '정확한',\n",
       "  '포인',\n",
       "  '팅',\n",
       "  '정보와',\n",
       "  '선택',\n",
       "  '적',\n",
       "  '복사',\n",
       "  '지시',\n",
       "  '정보를',\n",
       "  '명시적',\n",
       "  '으로',\n",
       "  '제공하는',\n",
       "  '방법으로',\n",
       "  '제안',\n",
       "  '하였다',\n",
       "  '학습',\n",
       "  '데이터는',\n",
       "  '논문의',\n",
       "  '초록과',\n",
       "  '제목을',\n",
       "  '대상',\n",
       "  '문서',\n",
       "  '와',\n",
       "  '정답',\n",
       "  '요약',\n",
       "  '으로',\n",
       "  '사용',\n",
       "  '하였다',\n",
       "  '제안한',\n",
       "  '인코딩',\n",
       "  '디코딩',\n",
       "  '기반',\n",
       "  '모델을',\n",
       "  '통해서',\n",
       "  '자동',\n",
       "  '생성',\n",
       "  '요약',\n",
       "  '을',\n",
       "  '수행',\n",
       "  '한',\n",
       "  '결과',\n",
       "  '단어',\n",
       "  '제현',\n",
       "  '기반의',\n",
       "  '1',\n",
       "  '이',\n",
       "  '4701',\n",
       "  '로',\n",
       "  '나타났으며',\n",
       "  '또한',\n",
       "  '어순',\n",
       "  '기반의',\n",
       "  '이',\n",
       "  '2955',\n",
       "  '로',\n",
       "  '향상',\n",
       "  '되었다']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_sent_token[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장과 단어 개수 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train + val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token = list(train_X) + list(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3740/3740 [00:00<00:00, 5316.13it/s]\n"
     ]
    }
   ],
   "source": [
    "word_len = []\n",
    "\n",
    "for sentences in tqdm(sent_token):\n",
    "    for sentence in sentences.split('. '):\n",
    "        word_len.append(sentence.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['그러므로',\n",
       " '선박,',\n",
       " '빌딩,',\n",
       " '기차,',\n",
       " '비행기',\n",
       " '등',\n",
       " 'Modbus를',\n",
       " '이용하는',\n",
       " '모든',\n",
       " '장비들과',\n",
       " '연결이',\n",
       " '가능하여',\n",
       " '환경변수의',\n",
       " '측정',\n",
       " '및',\n",
       " '원격제어가',\n",
       " '가능하게',\n",
       " '된다']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_len[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3740/3740 [00:00<00:00, 8801.56it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_num = []\n",
    "\n",
    "for sentences in tqdm(sent_token):\n",
    "    sentence_num.append(sentences.split('. '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Modbus는 각종 자동화 장비 감시 및 제어에 전 세계적으로 널리 사용되고 있는 자발적 산업표준 통신 프로토콜이다',\n",
       " '그러므로 선박, 빌딩, 기차, 비행기 등 Modbus를 이용하는 모든 장비들과 연결이 가능하여 환경변수의 측정 및 원격제어가 가능하게 된다',\n",
       " '본 논문에서 는 퍼지제어 시스템을 이용하여 외부환경요인을 각각 조합한 불확실한 내용을 정량적인 값으로 변환하여 LED 조명으로 표현하기 위해 알고 리즘을 설계하고, 설계한 알고리즘에 Modbus 통신 프로토콜을 추가하여 선박의 통합관리 시스템에서 외부환경요인 확인 및 원격제어가 가능 한 감성조명용 LED 제어기 회로를 설계 및 구현 하였다',\n",
       " '외부환경요소인 온도, 습도, 조도 값을 센서를 통해 제어기로 받아들이고 이 값들을 퍼지제어 알고리즘을 통해 LED로 표현된다',\n",
       " 'Modbus는 Serial 통신으로 RS485를 이용하여 다른 기기와 연결 되어 온도, 습도, 조도 상태 및 LED 출력 값 확인이 가능하고 또한 사용자가 원격으로 RGB 값을 변경 할 수 있기 때문에 원하는 색으로 변경이 가능하게 된다',\n",
       " '제작한 제 어기로 온도, 습도, 조도에 따라 LED 조명색상이 변화 되는 것을 확인 하였다.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_num[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 내의 최대 문장 개수:  34\n",
      "문서 내의 최소 문장 개수:  5\n",
      "문서 내의 평균 문장 개수 : 7.831550802139038\n",
      "문서 내의 문장 개수 중앙값 : 7.0\n"
     ]
    }
   ],
   "source": [
    "print('문서 내의 최대 문장 개수: ', max([len(i) for i in sentence_num]))\n",
    "print('문서 내의 최소 문장 개수: ', min([len(i) for i in sentence_num]))\n",
    "print('문서 내의 평균 문장 개수 :', sum(map(len, sentence_num))/len(sentence_num))\n",
    "print('문서 내의 문장 개수 중앙값 :', np.median([len(i) for i in sentence_num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 내의 최대 단어 개수:  391\n",
      "문장 내의 최소 단어 개수:  1\n",
      "문장 내의 평균 단어 개수 : 19.31136906794128\n",
      "문장 내의 단어 개수 중앙값 : 18.0\n"
     ]
    }
   ],
   "source": [
    "print('문장 내의 최대 단어 개수: ', max([len(j) for j in word_len]))\n",
    "print('문장 내의 최소 단어 개수: ', min([len(j) for j in word_len]))\n",
    "print('문장 내의 평균 단어 개수 :', sum(map(len, word_len))/len(word_len))\n",
    "print('문장 내의 단어 개수 중앙값 :', np.median([len(j) for j in word_len]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCES = 20\n",
    "MAX_SENTENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_sent_token = train_sent_token + val_sent_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X_data.shape: (2805, 20, 200)\n",
      "train_Y_data.shape: (2805, 4)\n",
      "val_X_data.shape: (935, 20, 200)\n",
      "val_Y_data.shape: (935, 4)\n",
      "test_X_data.shape: (935, 20, 200)\n",
      "test_Y_data.shape: (935, 4)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_val_sent_token)\n",
    "\n",
    "\n",
    "max_nb_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "def doc2hierarchical(text, max_sentences = MAX_SENTENCES, max_sentence_length = MAX_SENTENCE_LENGTH):\n",
    "    sentences = text.split('. ')\n",
    "    tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "    tokenized_sentences = pad_sequences(tokenized_sentences, maxlen = max_sentence_length)\n",
    "\n",
    "    pad_size = max_sentences - tokenized_sentences.shape[0]\n",
    "\n",
    "    if pad_size <= 0:  # tokenized_sentences.shape[0] < max_sentences\n",
    "        tokenized_sentences = tokenized_sentences[:max_sentences]\n",
    "    else:\n",
    "        tokenized_sentences = np.pad(tokenized_sentences, ((0, pad_size), (0, 0)), mode='constant', constant_values=0)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "            \n",
    "def build_dataset(x_data, y_data, max_sentences = MAX_SENTENCES, max_sentence_length = MAX_SENTENCE_LENGTH, tokenizer = tokenizer):\n",
    "    nb_instances = len(x_data)\n",
    "    X_data = np.zeros((nb_instances, max_sentences, max_sentence_length), dtype='int32')\n",
    "    for i, review in enumerate(x_data):\n",
    "        tokenized_sentences = doc2hierarchical(review)\n",
    "            \n",
    "        X_data[i] = tokenized_sentences[None, ...]\n",
    "        \n",
    "    nb_classes = y_data\n",
    "    Y_data = nb_classes #to_categorical(y_data, nb_classes)\n",
    "    \n",
    "    return X_data, Y_data\n",
    "\n",
    "\n",
    "train_X_data, train_Y_data = build_dataset(train_X, train_y)\n",
    "val_X_data, val_Y_data = build_dataset(val_X, val_y)\n",
    "test_X_data, test_Y_data = build_dataset(test_X, test_y)\n",
    "\n",
    "print(\"train_X_data.shape: {}\".format(train_X_data.shape))\n",
    "print(\"train_Y_data.shape: {}\".format(train_Y_data.shape))\n",
    "print(\"val_X_data.shape: {}\".format(val_X_data.shape))\n",
    "print(\"val_Y_data.shape: {}\".format(val_Y_data.shape))\n",
    "print(\"test_X_data.shape: {}\".format(test_X_data.shape))\n",
    "print(\"test_Y_data.shape: {}\".format(test_Y_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'의': 1,\n",
       " '을': 2,\n",
       " '에': 3,\n",
       " '이': 4,\n",
       " '수': 5,\n",
       " '있다': 6,\n",
       " '하였다': 7,\n",
       " '를': 8,\n",
       " '한다': 9,\n",
       " '하고': 10,\n",
       " '할': 11,\n",
       " '본': 12,\n",
       " '대한': 13,\n",
       " '과': 14,\n",
       " '적': 15,\n",
       " '및': 16,\n",
       " '는': 17,\n",
       " '하는': 18,\n",
       " '있는': 19,\n",
       " '한': 20,\n",
       " '은': 21,\n",
       " '가': 22,\n",
       " '으로': 23,\n",
       " '것이': 24,\n",
       " '와': 25,\n",
       " '위해': 26,\n",
       " '통해': 27,\n",
       " '된': 28,\n",
       " '인': 29,\n",
       " '위한': 30,\n",
       " '로': 31,\n",
       " '인공지능': 32,\n",
       " '분석': 33,\n",
       " '하기': 34,\n",
       " '다': 35,\n",
       " '학습': 36,\n",
       " '들': 37,\n",
       " '이다': 38,\n",
       " '연구': 39,\n",
       " '에서': 40,\n",
       " '그': 41,\n",
       " '활용': 42,\n",
       " '이러한': 43,\n",
       " '다양한': 44,\n",
       " '이를': 45,\n",
       " '4': 46,\n",
       " '하여': 47,\n",
       " '것으로': 48,\n",
       " '교육': 49,\n",
       " '연구는': 50,\n",
       " '기술': 51,\n",
       " '고': 52,\n",
       " '제안': 53,\n",
       " '될': 54,\n",
       " '차': 55,\n",
       " '적용': 56,\n",
       " '또한': 57,\n",
       " '데이터': 58,\n",
       " '개발': 59,\n",
       " '사용': 60,\n",
       " '결과': 61,\n",
       " '된다': 62,\n",
       " '새로운': 63,\n",
       " '산업혁명': 64,\n",
       " '따라': 65,\n",
       " '예측': 66,\n",
       " '되고': 67,\n",
       " '경우': 68,\n",
       " '관련': 69,\n",
       " '도': 70,\n",
       " '되는': 71,\n",
       " '스마트': 72,\n",
       " '연구에서는': 73,\n",
       " '그리고': 74,\n",
       " '정보': 75,\n",
       " '되었다': 76,\n",
       " '기반': 77,\n",
       " '대해': 78,\n",
       " '인간': 79,\n",
       " '제': 80,\n",
       " '등': 81,\n",
       " '논문에서는': 82,\n",
       " '데이터를': 83,\n",
       " '영향을': 84,\n",
       " '제시': 85,\n",
       " '같은': 86,\n",
       " '최근': 87,\n",
       " '이에': 88,\n",
       " '3': 89,\n",
       " '분류': 90,\n",
       " '따라서': 91,\n",
       " '해': 92,\n",
       " '가장': 93,\n",
       " '이용하여': 94,\n",
       " '서': 95,\n",
       " '수행': 96,\n",
       " '확인': 97,\n",
       " '평가': 98,\n",
       " '인간의': 99,\n",
       " '되어': 100,\n",
       " '보다': 101,\n",
       " '사물인터넷': 102,\n",
       " '중': 103,\n",
       " '가지': 104,\n",
       " '연구의': 105,\n",
       " '특히': 106,\n",
       " '지': 107,\n",
       " '성': 108,\n",
       " '위하여': 109,\n",
       " '서비스': 110,\n",
       " '문제': 111,\n",
       " '기존': 112,\n",
       " '2': 113,\n",
       " '정보를': 114,\n",
       " '있었다': 115,\n",
       " '융합': 116,\n",
       " '인식': 117,\n",
       " '미래': 118,\n",
       " '제공': 119,\n",
       " '있으며': 120,\n",
       " '수업': 121,\n",
       " '나타났다': 122,\n",
       " '결과를': 123,\n",
       " '하고자': 124,\n",
       " '발생': 125,\n",
       " '설계': 126,\n",
       " '모델을': 127,\n",
       " '높은': 128,\n",
       " '것을': 129,\n",
       " '더': 130,\n",
       " '핵심': 131,\n",
       " '그러나': 132,\n",
       " '따른': 133,\n",
       " '비교': 134,\n",
       " '많은': 135,\n",
       " '아니라': 136,\n",
       " '방법을': 137,\n",
       " '대상으로': 138,\n",
       " '사회': 139,\n",
       " '관한': 140,\n",
       " '기존의': 141,\n",
       " '여': 142,\n",
       " '디자인': 143,\n",
       " '증가': 144,\n",
       " '진행': 145,\n",
       " '생성': 146,\n",
       " '해결': 147,\n",
       " '등의': 148,\n",
       " '산업': 149,\n",
       " '해야': 150,\n",
       " '기술의': 151,\n",
       " '각': 152,\n",
       " '인공지능의': 153,\n",
       " '기반으로': 154,\n",
       " '실험': 155,\n",
       " '변화': 156,\n",
       " '현재': 157,\n",
       " '구성': 158,\n",
       " '있을': 159,\n",
       " '하였으며': 160,\n",
       " '성능을': 161,\n",
       " '다른': 162,\n",
       " '에서는': 163,\n",
       " '발전': 164,\n",
       " '빅데이터': 165,\n",
       " '통한': 166,\n",
       " '기반의': 167,\n",
       " '1': 168,\n",
       " '바탕으로': 169,\n",
       " '딥러닝': 170,\n",
       " '활용하여': 171,\n",
       " '실제': 172,\n",
       " '연구가': 173,\n",
       " '미치는': 174,\n",
       " '문제를': 175,\n",
       " '기술을': 176,\n",
       " '등을': 177,\n",
       " '통하여': 178,\n",
       " '때': 179,\n",
       " '활용한': 180,\n",
       " '위해서는': 181,\n",
       " '개선': 182,\n",
       " '분석을': 183,\n",
       " '둘째': 184,\n",
       " '첫째': 185,\n",
       " '논문은': 186,\n",
       " '때문에': 187,\n",
       " '디지털': 188,\n",
       " '중요한': 189,\n",
       " '검토': 190,\n",
       " '수집': 191,\n",
       " '이용한': 192,\n",
       " '공학': 193,\n",
       " '의미': 194,\n",
       " '향후': 195,\n",
       " '필요': 196,\n",
       " '음악': 197,\n",
       " '논의': 198,\n",
       " '중심으로': 199,\n",
       " '도출': 200,\n",
       " '며': 201,\n",
       " '향상': 202,\n",
       " '이용': 203,\n",
       " '사회적': 204,\n",
       " '접근': 205,\n",
       " '후': 206,\n",
       " '시스템을': 207,\n",
       " '개의': 208,\n",
       " '연구를': 209,\n",
       " '간': 210,\n",
       " '했다': 211,\n",
       " '화': 212,\n",
       " '하지만': 213,\n",
       " '에는': 214,\n",
       " '필요하다': 215,\n",
       " '검증': 216,\n",
       " '에게': 217,\n",
       " '구축': 218,\n",
       " '주요': 219,\n",
       " '자': 220,\n",
       " '개': 221,\n",
       " '네트워크': 222,\n",
       " '사용자': 223,\n",
       " '필요한': 224,\n",
       " '있어': 225,\n",
       " '두': 226,\n",
       " '관리': 227,\n",
       " '기계학습': 228,\n",
       " '있도록': 229,\n",
       " '기초': 230,\n",
       " '이는': 231,\n",
       " '국내': 232,\n",
       " '시스템': 233,\n",
       " '하는데': 234,\n",
       " '이나': 235,\n",
       " '여러': 236,\n",
       " '인공지능이': 237,\n",
       " '다음': 238,\n",
       " '기법을': 239,\n",
       " '사물': 240,\n",
       " '것은': 241,\n",
       " '함께': 242,\n",
       " '존재': 243,\n",
       " '함으로써': 244,\n",
       " '교육의': 245,\n",
       " '한국': 246,\n",
       " '교수': 247,\n",
       " '하였고': 248,\n",
       " '과학': 249,\n",
       " '개인': 250,\n",
       " '측정': 251,\n",
       " '방안을': 252,\n",
       " '고려': 253,\n",
       " '나': 254,\n",
       " '판단': 255,\n",
       " '성능': 256,\n",
       " '법적': 257,\n",
       " '매우': 258,\n",
       " '관련된': 259,\n",
       " '알고리즘': 260,\n",
       " '어떻게': 261,\n",
       " '대하여': 262,\n",
       " '로봇': 263,\n",
       " '구현': 264,\n",
       " '하며': 265,\n",
       " '셋째': 266,\n",
       " '큰': 267,\n",
       " '모든': 268,\n",
       " '지식': 269,\n",
       " '있다는': 270,\n",
       " '도입': 271,\n",
       " '이미지': 272,\n",
       " '또는': 273,\n",
       " '인해': 274,\n",
       " '보안': 275,\n",
       " '운영': 276,\n",
       " '처리': 277,\n",
       " '되어야': 278,\n",
       " '개인정보': 279,\n",
       " '변화를': 280,\n",
       " '조사': 281,\n",
       " '지원': 282,\n",
       " '형': 283,\n",
       " '대학': 284,\n",
       " '요구': 285,\n",
       " '기계': 286,\n",
       " '특성을': 287,\n",
       " '데이터의': 288,\n",
       " '사용하여': 289,\n",
       " '탐색': 290,\n",
       " '알고리즘을': 291,\n",
       " '생각': 292,\n",
       " '실시': 293,\n",
       " '지능': 294,\n",
       " '연결': 295,\n",
       " '있어서': 296,\n",
       " '하면서': 297,\n",
       " '방법': 298,\n",
       " '개념': 299,\n",
       " '해당': 300,\n",
       " '센서': 301,\n",
       " '가능한': 302,\n",
       " '5': 303,\n",
       " '적용하여': 304,\n",
       " '특징': 305,\n",
       " '가능성을': 306,\n",
       " '환경': 307,\n",
       " '있음을': 308,\n",
       " '대': 309,\n",
       " '사전': 310,\n",
       " '영상': 311,\n",
       " '선택': 312,\n",
       " '게': 313,\n",
       " '확장': 314,\n",
       " '간의': 315,\n",
       " '국가': 316,\n",
       " '있고': 317,\n",
       " '인공지능에': 318,\n",
       " '없는': 319,\n",
       " '인터넷': 320,\n",
       " '통합': 321,\n",
       " '대응': 322,\n",
       " '과정에서': 323,\n",
       " '이고': 324,\n",
       " '현실': 325,\n",
       " '보였다': 326,\n",
       " '모두': 327,\n",
       " '이해': 328,\n",
       " '하지': 329,\n",
       " '결과는': 330,\n",
       " '역할을': 331,\n",
       " '관점에서': 332,\n",
       " '기술이': 333,\n",
       " '소프트웨어': 334,\n",
       " '모델': 335,\n",
       " '즉': 336,\n",
       " '비해': 337,\n",
       " '감성': 338,\n",
       " '참여': 339,\n",
       " '표현': 340,\n",
       " '시': 341,\n",
       " '포함': 342,\n",
       " '규제': 343,\n",
       " '에서의': 344,\n",
       " '학교': 345,\n",
       " '보호': 346,\n",
       " '인공지능을': 347,\n",
       " '인지': 348,\n",
       " '서비스를': 349,\n",
       " '의한': 350,\n",
       " '프로그램': 351,\n",
       " '내용': 352,\n",
       " '텍스트': 353,\n",
       " '일반': 354,\n",
       " '정의': 355,\n",
       " '등장': 356,\n",
       " '모형을': 357,\n",
       " '문제점': 358,\n",
       " '생산': 359,\n",
       " '경험': 360,\n",
       " '제안하는': 361,\n",
       " '체계': 362,\n",
       " '만': 363,\n",
       " '분야에서': 364,\n",
       " '데': 365,\n",
       " '부터': 366,\n",
       " '라는': 367,\n",
       " '자료를': 368,\n",
       " '전공': 369,\n",
       " '기능': 370,\n",
       " '이와': 371,\n",
       " '추출': 372,\n",
       " '과정을': 373,\n",
       " '심층': 374,\n",
       " '전체': 375,\n",
       " '연구에서': 376,\n",
       " '시스템의': 377,\n",
       " '까지': 378,\n",
       " '의해': 379,\n",
       " '정책': 380,\n",
       " '목적은': 381,\n",
       " '활동': 382,\n",
       " '컴퓨터': 383,\n",
       " '확보': 384,\n",
       " '결정': 385,\n",
       " '설명': 386,\n",
       " '크게': 387,\n",
       " '볼': 388,\n",
       " '모델의': 389,\n",
       " '상호작용': 390,\n",
       " '이며': 391,\n",
       " '때문': 392,\n",
       " '어떤': 393,\n",
       " '환경에서': 394,\n",
       " '자기': 395,\n",
       " '가지고': 396,\n",
       " '세': 397,\n",
       " '역량': 398,\n",
       " '전': 399,\n",
       " '중심': 400,\n",
       " '더욱': 401,\n",
       " '뿐': 402,\n",
       " '사용자의': 403,\n",
       " '상황': 404,\n",
       " '한계': 405,\n",
       " '창의적': 406,\n",
       " '제품': 407,\n",
       " '우리': 408,\n",
       " '먼저': 409,\n",
       " '분야': 410,\n",
       " '의료': 411,\n",
       " '학년': 412,\n",
       " '온라인': 413,\n",
       " '확대': 414,\n",
       " '세계': 415,\n",
       " '측면에서': 416,\n",
       " '예술': 417,\n",
       " '교육을': 418,\n",
       " '이라는': 419,\n",
       " '전문가': 420,\n",
       " '인간과': 421,\n",
       " '특성': 422,\n",
       " '콘텐츠': 423,\n",
       " '시대에': 424,\n",
       " '행동': 425,\n",
       " '효과를': 426,\n",
       " '정확도를': 427,\n",
       " '관계': 428,\n",
       " '사고': 429,\n",
       " '머신러닝': 430,\n",
       " '점에서': 431,\n",
       " '분야의': 432,\n",
       " '디바이스': 433,\n",
       " '한다는': 434,\n",
       " '시도': 435,\n",
       " '모바일': 436,\n",
       " '교과': 437,\n",
       " '총': 438,\n",
       " '내용을': 439,\n",
       " '제작': 440,\n",
       " '탐지': 441,\n",
       " '분석하여': 442,\n",
       " '야': 443,\n",
       " '문화': 444,\n",
       " '공간': 445,\n",
       " '관계를': 446,\n",
       " '평균': 447,\n",
       " '마련': 448,\n",
       " '자료': 449,\n",
       " '인공지능과': 450,\n",
       " '같다': 451,\n",
       " '입력': 452,\n",
       " '구': 453,\n",
       " '문제가': 454,\n",
       " '자동': 455,\n",
       " '학생들의': 456,\n",
       " '직접': 457,\n",
       " '토대로': 458,\n",
       " '영향': 459,\n",
       " '학생': 460,\n",
       " '등이': 461,\n",
       " '제공하는': 462,\n",
       " '기술적': 463,\n",
       " '시대의': 464,\n",
       " '않은': 465,\n",
       " '차이가': 466,\n",
       " '제안한': 467,\n",
       " '파악': 468,\n",
       " '이상': 469,\n",
       " '이후': 470,\n",
       " '우리나라': 471,\n",
       " '하게': 472,\n",
       " '시작': 473,\n",
       " '제안된': 474,\n",
       " '현': 475,\n",
       " '유의': 476,\n",
       " '학습을': 477,\n",
       " '방향을': 478,\n",
       " '감정': 479,\n",
       " '형성': 480,\n",
       " '신경망': 481,\n",
       " '별': 482,\n",
       " '많이': 483,\n",
       " '같이': 484,\n",
       " '사이버': 485,\n",
       " '위치': 486,\n",
       " '동시에': 487,\n",
       " '인한': 488,\n",
       " '되지': 489,\n",
       " '화된': 490,\n",
       " '기업': 491,\n",
       " '프로그램을': 492,\n",
       " '측면': 493,\n",
       " '에도': 494,\n",
       " '혁신': 495,\n",
       " '방법은': 496,\n",
       " '유형': 497,\n",
       " '적합한': 498,\n",
       " '실험을': 499,\n",
       " '변화에': 500,\n",
       " '교사': 501,\n",
       " '시스템은': 502,\n",
       " '법률': 503,\n",
       " '플랫폼': 504,\n",
       " '연계': 505,\n",
       " '영역': 506,\n",
       " '기대': 507,\n",
       " '사례를': 508,\n",
       " '마지막으로': 509,\n",
       " '의사결정': 510,\n",
       " '이루어지': 511,\n",
       " '어떠한': 512,\n",
       " '상호': 513,\n",
       " '상': 514,\n",
       " '선정': 515,\n",
       " '설정': 516,\n",
       " '제어': 517,\n",
       " '면': 518,\n",
       " '개별': 519,\n",
       " '가치': 520,\n",
       " '등에': 521,\n",
       " '고찰': 522,\n",
       " '알': 523,\n",
       " '통신': 524,\n",
       " '단어': 525,\n",
       " '기능을': 526,\n",
       " '관련하여': 527,\n",
       " '컴퓨팅': 528,\n",
       " '스마트폰': 529,\n",
       " '언어': 530,\n",
       " '전문': 531,\n",
       " '특정': 532,\n",
       " '시킬': 533,\n",
       " '계산': 534,\n",
       " '인공지능은': 535,\n",
       " '결합': 536,\n",
       " '우선': 537,\n",
       " '교수학습': 538,\n",
       " '과제': 539,\n",
       " '수학': 540,\n",
       " '었다': 541,\n",
       " '학습자': 542,\n",
       " '더불어': 543,\n",
       " '로봇의': 544,\n",
       " '발달': 545,\n",
       " '능력': 546,\n",
       " '점을': 547,\n",
       " '있지만': 548,\n",
       " '내': 549,\n",
       " '감소': 550,\n",
       " '모형': 551,\n",
       " '시간': 552,\n",
       " '인공': 553,\n",
       " '창의성': 554,\n",
       " '문장': 555,\n",
       " '모색': 556,\n",
       " '중국': 557,\n",
       " '방식을': 558,\n",
       " '책임': 559,\n",
       " '라고': 560,\n",
       " '금융': 561,\n",
       " '않고': 562,\n",
       " '분야에': 563,\n",
       " '글쓰기': 564,\n",
       " '보인다': 565,\n",
       " '발생하는': 566,\n",
       " '효과적인': 567,\n",
       " '전자': 568,\n",
       " '구조': 569,\n",
       " '특징을': 570,\n",
       " '없이': 571,\n",
       " '차원': 572,\n",
       " '나아가': 573,\n",
       " '지능형': 574,\n",
       " '협력': 575,\n",
       " '가치를': 576,\n",
       " '응용': 577,\n",
       " '효과적으로': 578,\n",
       " '공유': 579,\n",
       " '부분': 580,\n",
       " '검색': 581,\n",
       " '확인할': 582,\n",
       " '아닌': 583,\n",
       " '기술은': 584,\n",
       " '없다': 585,\n",
       " '소비자': 586,\n",
       " '에서도': 587,\n",
       " '권리': 588,\n",
       " '시대': 589,\n",
       " '주로': 590,\n",
       " '교육과정': 591,\n",
       " '자동차': 592,\n",
       " '클라우드': 593,\n",
       " '기본': 594,\n",
       " '위험': 595,\n",
       " '검출': 596,\n",
       " '사례': 597,\n",
       " '단계': 598,\n",
       " '약': 599,\n",
       " '살펴보았다': 600,\n",
       " '러닝': 601,\n",
       " '하도록': 602,\n",
       " '능력을': 603,\n",
       " '하거나': 604,\n",
       " '잘': 605,\n",
       " '목적으로': 606,\n",
       " '성과': 607,\n",
       " '우리는': 608,\n",
       " '가상': 609,\n",
       " '중심의': 610,\n",
       " '윤리적': 611,\n",
       " '확산': 612,\n",
       " '정보의': 613,\n",
       " '윤리': 614,\n",
       " '과의': 615,\n",
       " '함에': 616,\n",
       " '있게': 617,\n",
       " '기술과': 618,\n",
       " '목적을': 619,\n",
       " '탐구': 620,\n",
       " '학생들이': 621,\n",
       " '진행되고': 622,\n",
       " '효과': 623,\n",
       " '혹은': 624,\n",
       " '미국': 625,\n",
       " '모델은': 626,\n",
       " '인간이': 627,\n",
       " '않는': 628,\n",
       " '고려하여': 629,\n",
       " '력': 630,\n",
       " '강조': 631,\n",
       " '과학기술': 632,\n",
       " '수용': 633,\n",
       " '시사점을': 634,\n",
       " '산업의': 635,\n",
       " '방법으로': 636,\n",
       " '목적': 637,\n",
       " '영화': 638,\n",
       " '인정': 639,\n",
       " '개인의': 640,\n",
       " '역할': 641,\n",
       " '객체': 642,\n",
       " '반영': 643,\n",
       " '추정': 644,\n",
       " '블록체인': 645,\n",
       " '스스로': 646,\n",
       " '6': 647,\n",
       " '현대': 648,\n",
       " '통해서': 649,\n",
       " '차량': 650,\n",
       " '창작': 651,\n",
       " '좋은': 652,\n",
       " '주장': 653,\n",
       " '활성화': 654,\n",
       " '중에서': 655,\n",
       " '기법': 656,\n",
       " '사용하는': 657,\n",
       " '연구결과': 658,\n",
       " '서로': 659,\n",
       " '요소를': 660,\n",
       " '실시간': 661,\n",
       " '진단': 662,\n",
       " '하면': 663,\n",
       " '강화': 664,\n",
       " '실행': 665,\n",
       " '문제해결': 666,\n",
       " '양성': 667,\n",
       " '과정': 668,\n",
       " '분석한': 669,\n",
       " '이런': 670,\n",
       " '방법이': 671,\n",
       " '모니터링': 672,\n",
       " '요소': 673,\n",
       " '사': 674,\n",
       " '속에서': 675,\n",
       " '상태': 676,\n",
       " '주목': 677,\n",
       " '향상을': 678,\n",
       " '역량을': 679,\n",
       " '이들': 680,\n",
       " '있다고': 681,\n",
       " '이론적': 682,\n",
       " '식': 683,\n",
       " '집중': 684,\n",
       " '이미': 685,\n",
       " '아직': 686,\n",
       " '추가': 687,\n",
       " '안전': 688,\n",
       " '개념을': 689,\n",
       " '한국어': 690,\n",
       " '평가를': 691,\n",
       " '줄': 692,\n",
       " '한편': 693,\n",
       " '유의미한': 694,\n",
       " '기업의': 695,\n",
       " '대상': 696,\n",
       " '가능성이': 697,\n",
       " '개발된': 698,\n",
       " '학습자의': 699,\n",
       " '하나의': 700,\n",
       " '있기': 701,\n",
       " '종합': 702,\n",
       " '웹': 703,\n",
       " '활용하는': 704,\n",
       " '접목': 705,\n",
       " '경제': 706,\n",
       " '시각': 707,\n",
       " '첨단': 708,\n",
       " '빅': 709,\n",
       " '현장': 710,\n",
       " '보조공학': 711,\n",
       " '가진': 712,\n",
       " '초기': 713,\n",
       " '되기': 714,\n",
       " '드론': 715,\n",
       " '모형의': 716,\n",
       " '교육에': 717,\n",
       " '개정': 718,\n",
       " '프로젝트': 719,\n",
       " '추천': 720,\n",
       " '창의': 721,\n",
       " '시장': 722,\n",
       " '각각': 723,\n",
       " '지역': 724,\n",
       " '것인지': 725,\n",
       " '자신의': 726,\n",
       " '변화가': 727,\n",
       " '주제': 728,\n",
       " '진로': 729,\n",
       " '구분': 730,\n",
       " '법': 731,\n",
       " '데이터에': 732,\n",
       " '기대한다': 733,\n",
       " '체험': 734,\n",
       " '우수한': 735,\n",
       " '배경': 736,\n",
       " '창출': 737,\n",
       " '어려운': 738,\n",
       " '방식으로': 739,\n",
       " '효율적인': 740,\n",
       " '실현': 741,\n",
       " '논문에서': 742,\n",
       " '되었으며': 743,\n",
       " '장애': 744,\n",
       " '대학의': 745,\n",
       " '낮은': 746,\n",
       " '불구하고': 747,\n",
       " '갖는': 748,\n",
       " '살펴보고': 749,\n",
       " '에너지': 750,\n",
       " '적절한': 751,\n",
       " '대비': 752,\n",
       " '한계를': 753,\n",
       " '데이터가': 754,\n",
       " '의미를': 755,\n",
       " '물론': 756,\n",
       " '통계적': 757,\n",
       " '보완': 758,\n",
       " '높게': 759,\n",
       " '예상': 760,\n",
       " '유사한': 761,\n",
       " '관점': 762,\n",
       " '국제': 763,\n",
       " '수정': 764,\n",
       " '교육이': 765,\n",
       " '자동화': 766,\n",
       " '달성': 767,\n",
       " '집단': 768,\n",
       " '도덕': 769,\n",
       " '왔다': 770,\n",
       " '기술에': 771,\n",
       " '전송': 772,\n",
       " '공공': 773,\n",
       " '앞으로': 774,\n",
       " '쟁점': 775,\n",
       " '수업을': 776,\n",
       " '해석': 777,\n",
       " '사회의': 778,\n",
       " '상황에서': 779,\n",
       " '수집된': 780,\n",
       " '아니': 781,\n",
       " '시뮬레이션': 782,\n",
       " '효율적으로': 783,\n",
       " '사람': 784,\n",
       " '대체': 785,\n",
       " '미디어': 786,\n",
       " '발견': 787,\n",
       " '명의': 788,\n",
       " '교육적': 789,\n",
       " '형태의': 790,\n",
       " '전통적인': 791,\n",
       " '부여': 792,\n",
       " '다중': 793,\n",
       " '서비스의': 794,\n",
       " '환경을': 795,\n",
       " '영역에서': 796,\n",
       " '추론': 797,\n",
       " '달리': 798,\n",
       " '필요성': 799,\n",
       " '형태로': 800,\n",
       " '하여야': 801,\n",
       " '최적화': 802,\n",
       " '되면서': 803,\n",
       " '차이를': 804,\n",
       " '유지': 805,\n",
       " '뿐만': 806,\n",
       " '패턴': 807,\n",
       " '관심이': 808,\n",
       " '설치': 809,\n",
       " '역시': 810,\n",
       " '구체적으로': 811,\n",
       " '방법에': 812,\n",
       " '피드백': 813,\n",
       " '지만': 814,\n",
       " '지식을': 815,\n",
       " '최종': 816,\n",
       " '그에': 817,\n",
       " '소통': 818,\n",
       " '포스트휴먼': 819,\n",
       " '기기': 820,\n",
       " '전망': 821,\n",
       " '신체': 822,\n",
       " '요인': 823,\n",
       " '노력': 824,\n",
       " '사이의': 825,\n",
       " '영역을': 826,\n",
       " '수준': 827,\n",
       " '등으로': 828,\n",
       " '10': 829,\n",
       " '이상의': 830,\n",
       " '도움이': 831,\n",
       " '으로는': 832,\n",
       " '생활': 833,\n",
       " '구체적인': 834,\n",
       " '상황을': 835,\n",
       " '발전에': 836,\n",
       " '넷째': 837,\n",
       " '있으나': 838,\n",
       " '소개': 839,\n",
       " '개발을': 840,\n",
       " '작성': 841,\n",
       " '긍정적인': 842,\n",
       " '침해': 843,\n",
       " '본질': 844,\n",
       " '전략을': 845,\n",
       " '무선': 846,\n",
       " '수준의': 847,\n",
       " '하나인': 848,\n",
       " '명을': 849,\n",
       " '식별': 850,\n",
       " '속': 851,\n",
       " '문제에': 852,\n",
       " '검사': 853,\n",
       " '논의가': 854,\n",
       " '만을': 855,\n",
       " '새롭게': 856,\n",
       " '계': 857,\n",
       " '게임': 858,\n",
       " '인하여': 859,\n",
       " '문서': 860,\n",
       " '현행': 861,\n",
       " '방식': 862,\n",
       " '우리의': 863,\n",
       " '연구에': 864,\n",
       " '것에': 865,\n",
       " '실천': 866,\n",
       " '빠르게': 867,\n",
       " '이용해': 868,\n",
       " '챗봇': 869,\n",
       " '속성': 870,\n",
       " '국내외': 871,\n",
       " '전략': 872,\n",
       " '활용되고': 873,\n",
       " '인식을': 874,\n",
       " '의의': 875,\n",
       " '대표적인': 876,\n",
       " '번째': 877,\n",
       " '추진': 878,\n",
       " '경제적': 879,\n",
       " '사용자가': 880,\n",
       " '점이': 881,\n",
       " '응답': 882,\n",
       " '시켜': 883,\n",
       " '지속적으로': 884,\n",
       " '대해서는': 885,\n",
       " '하나': 886,\n",
       " '군집': 887,\n",
       " '현재의': 888,\n",
       " '도시': 889,\n",
       " '관심을': 890,\n",
       " '근거': 891,\n",
       " '보았다': 892,\n",
       " '직업': 893,\n",
       " '선행': 894,\n",
       " '논의를': 895,\n",
       " '기': 896,\n",
       " '변수': 897,\n",
       " '특허': 898,\n",
       " '되며': 899,\n",
       " '의사소통': 900,\n",
       " '않다': 901,\n",
       " '제한': 902,\n",
       " '파악하고': 903,\n",
       " '데이터는': 904,\n",
       " '알고리즘의': 905,\n",
       " '작동': 906,\n",
       " '도로': 907,\n",
       " '방법의': 908,\n",
       " '실정': 909,\n",
       " '표준': 910,\n",
       " '구조를': 911,\n",
       " '긍정적': 912,\n",
       " '과거': 913,\n",
       " '주는': 914,\n",
       " '대부분': 915,\n",
       " '함을': 916,\n",
       " '진화': 917,\n",
       " '문제는': 918,\n",
       " '하려는': 919,\n",
       " '도덕적': 920,\n",
       " '반면': 921,\n",
       " '중요하다': 922,\n",
       " '경험을': 923,\n",
       " '상대적으로': 924,\n",
       " '딥': 925,\n",
       " '작업': 926,\n",
       " '전력': 927,\n",
       " '저장': 928,\n",
       " '동작': 929,\n",
       " '인공신경망': 930,\n",
       " '필요성이': 931,\n",
       " '테스트': 932,\n",
       " '생명': 933,\n",
       " '도출하': 934,\n",
       " '환경에': 935,\n",
       " '목적이': 936,\n",
       " '시키기': 937,\n",
       " '성능이': 938,\n",
       " '지적': 939,\n",
       " '활발히': 940,\n",
       " '오늘날': 941,\n",
       " '마음': 942,\n",
       " '자연': 943,\n",
       " '테크놀로지': 944,\n",
       " '규정': 945,\n",
       " '오류': 946,\n",
       " '정확한': 947,\n",
       " '것': 948,\n",
       " '훈련': 949,\n",
       " '목표': 950,\n",
       " '증대': 951,\n",
       " '이미지를': 952,\n",
       " '값을': 953,\n",
       " '어느': 954,\n",
       " '보장': 955,\n",
       " '논문': 956,\n",
       " '일부': 957,\n",
       " '으로서': 958,\n",
       " '실시간으로': 959,\n",
       " '프라이버시': 960,\n",
       " '받고': 961,\n",
       " '학습에': 962,\n",
       " '보다는': 963,\n",
       " '시대를': 964,\n",
       " '포함한': 965,\n",
       " '인성': 966,\n",
       " '교육공학': 967,\n",
       " '공할': 968,\n",
       " '20': 969,\n",
       " '기반한': 970,\n",
       " '웨어러블': 971,\n",
       " '시키는': 972,\n",
       " '서버': 973,\n",
       " '준비': 974,\n",
       " '기준': 975,\n",
       " '되었고': 976,\n",
       " '촉진': 977,\n",
       " '시키고': 978,\n",
       " '인간을': 979,\n",
       " '번역': 980,\n",
       " '프로그래밍': 981,\n",
       " '분석하는': 982,\n",
       " '문헌': 983,\n",
       " '수도': 984,\n",
       " '영역에': 985,\n",
       " '자율': 986,\n",
       " '점': 987,\n",
       " '제조': 988,\n",
       " '고려한': 989,\n",
       " '현장에서': 990,\n",
       " '파악하': 991,\n",
       " '입법': 992,\n",
       " '증강현실': 993,\n",
       " '야기': 994,\n",
       " '도움을': 995,\n",
       " '계획': 996,\n",
       " '책임을': 997,\n",
       " '갖고': 998,\n",
       " '투자': 999,\n",
       " '비교하여': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 41225 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('./data/embedding/word2vec_okt.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x1e18ce6dcc8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = model.wv\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    if word in word_vectors:\n",
    "        return word_vectors[word]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total absent words are 7 which is 0.02 % of total words\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\n",
    "absent_words = 0\n",
    "for word, i in word_index.items():\n",
    "    tmp = get_vector(word)\n",
    "    if tmp is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = tmp\n",
    "    else:\n",
    "        absent_words += 1\n",
    "print('Total absent words are', absent_words, 'which is', \"%0.2f\" % (absent_words * 100 / len(word_index)), '% of total words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import Bidirectional, TimeDistributed, LSTM, Conv1D, MaxPooling1D\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "from keras.layers import Lambda, Permute, RepeatVector, Multiply\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'W_regularizer': self.W_regularizer,\n",
    "            'u_regularizer': self.u_regularizer,\n",
    "            'b_regularizer': self.b_regularizer,\n",
    "            'W_constraint': self.W_constraint,\n",
    "            'u_constraint': self.u_constraint,\n",
    "            'b_constraint': self.b_constraint,\n",
    "            'bias': self.bias\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatibl|e with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "REG_PARAM = 1e-13\n",
    "l2_reg = regularizers.l2(REG_PARAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights = [embedding_matrix],\n",
    "                            input_length = MAX_SENTENCE_LENGTH,\n",
    "                            trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2805 samples, validate on 935 samples\n",
      "Epoch 1/30\n",
      "2805/2805 [==============================] - 84s 30ms/step - loss: 5.8284 - val_loss: 7.4491\n",
      "\n",
      "Epoch 00001: saving model to ./save_models/han_rae_ls4_v1_01_7.44908.h5\n",
      "Epoch 2/30\n",
      "2805/2805 [==============================] - 83s 29ms/step - loss: 4.6391 - val_loss: 6.5560\n",
      "\n",
      "Epoch 00002: saving model to ./save_models/han_rae_ls4_v1_02_6.55604.h5\n",
      "Epoch 3/30\n",
      "2805/2805 [==============================] - 83s 30ms/step - loss: 4.1002 - val_loss: 6.0492\n",
      "\n",
      "Epoch 00003: saving model to ./save_models/han_rae_ls4_v1_03_6.04920.h5\n",
      "Epoch 4/30\n",
      "2805/2805 [==============================] - 83s 29ms/step - loss: 3.6730 - val_loss: 4.3109\n",
      "\n",
      "Epoch 00004: saving model to ./save_models/han_rae_ls4_v1_04_4.31085.h5\n",
      "Epoch 5/30\n",
      "2805/2805 [==============================] - 83s 30ms/step - loss: 3.3297 - val_loss: 4.8707\n",
      "\n",
      "Epoch 00005: saving model to ./save_models/han_rae_ls4_v1_05_4.87075.h5\n",
      "Epoch 6/30\n",
      "2805/2805 [==============================] - 83s 30ms/step - loss: 3.0093 - val_loss: 4.2704\n",
      "\n",
      "Epoch 00006: saving model to ./save_models/han_rae_ls4_v1_06_4.27039.h5\n",
      "Epoch 7/30\n",
      "2805/2805 [==============================] - 83s 30ms/step - loss: 2.6806 - val_loss: 4.2485\n",
      "\n",
      "Epoch 00007: saving model to ./save_models/han_rae_ls4_v1_07_4.24848.h5\n",
      "Epoch 8/30\n",
      "2805/2805 [==============================] - 82s 29ms/step - loss: 2.4649 - val_loss: 4.4101\n",
      "\n",
      "Epoch 00008: saving model to ./save_models/han_rae_ls4_v1_08_4.41010.h5\n",
      "Epoch 9/30\n",
      "2805/2805 [==============================] - 82s 29ms/step - loss: 2.2419 - val_loss: 4.3199\n",
      "\n",
      "Epoch 00009: saving model to ./save_models/han_rae_ls4_v1_09_4.31994.h5\n",
      "Epoch 10/30\n",
      "2805/2805 [==============================] - 82s 29ms/step - loss: 2.0270 - val_loss: 4.2055\n",
      "\n",
      "Epoch 00010: saving model to ./save_models/han_rae_ls4_v1_10_4.20553.h5\n",
      "Epoch 11/30\n",
      "2805/2805 [==============================] - 82s 29ms/step - loss: 1.8984 - val_loss: 4.0892\n",
      "\n",
      "Epoch 00011: saving model to ./save_models/han_rae_ls4_v1_11_4.08916.h5\n",
      "Epoch 12/30\n",
      "2805/2805 [==============================] - 81s 29ms/step - loss: 1.7624 - val_loss: 4.2368\n",
      "\n",
      "Epoch 00012: saving model to ./save_models/han_rae_ls4_v1_12_4.23677.h5\n",
      "Epoch 13/30\n",
      "2805/2805 [==============================] - 82s 29ms/step - loss: 1.6217 - val_loss: 4.0328\n",
      "\n",
      "Epoch 00013: saving model to ./save_models/han_rae_ls4_v1_13_4.03284.h5\n",
      "Epoch 14/30\n",
      "2805/2805 [==============================] - 81s 29ms/step - loss: 1.4810 - val_loss: 4.6413\n",
      "\n",
      "Epoch 00014: saving model to ./save_models/han_rae_ls4_v1_14_4.64135.h5\n",
      "Epoch 15/30\n",
      "2805/2805 [==============================] - 82s 29ms/step - loss: 1.4174 - val_loss: 4.2465\n",
      "\n",
      "Epoch 00015: saving model to ./save_models/han_rae_ls4_v1_15_4.24650.h5\n",
      "Epoch 16/30\n",
      "2805/2805 [==============================] - 81s 29ms/step - loss: 1.3382 - val_loss: 4.3077\n",
      "\n",
      "Epoch 00016: saving model to ./save_models/han_rae_ls4_v1_16_4.30773.h5\n",
      "Epoch 17/30\n",
      "2805/2805 [==============================] - 82s 29ms/step - loss: 1.2652 - val_loss: 4.3863\n",
      "\n",
      "Epoch 00017: saving model to ./save_models/han_rae_ls4_v1_17_4.38635.h5\n",
      "Epoch 18/30\n",
      "2805/2805 [==============================] - 82s 29ms/step - loss: 1.1949 - val_loss: 3.9559\n",
      "\n",
      "Epoch 00018: saving model to ./save_models/han_rae_ls4_v1_18_3.95589.h5\n",
      "Epoch 19/30\n",
      "2805/2805 [==============================] - 82s 29ms/step - loss: 1.0982 - val_loss: 4.5660\n",
      "\n",
      "Epoch 00019: saving model to ./save_models/han_rae_ls4_v1_19_4.56598.h5\n",
      "Epoch 20/30\n",
      "2805/2805 [==============================] - 81s 29ms/step - loss: 1.0293 - val_loss: 3.9871\n",
      "\n",
      "Epoch 00020: saving model to ./save_models/han_rae_ls4_v1_20_3.98706.h5\n",
      "Epoch 21/30\n",
      "2805/2805 [==============================] - 82s 29ms/step - loss: 1.0718 - val_loss: 3.9423\n",
      "\n",
      "Epoch 00021: saving model to ./save_models/han_rae_ls4_v1_21_3.94225.h5\n",
      "Epoch 22/30\n",
      "2805/2805 [==============================] - 82s 29ms/step - loss: 1.0444 - val_loss: 4.2023\n",
      "\n",
      "Epoch 00022: saving model to ./save_models/han_rae_ls4_v1_22_4.20234.h5\n",
      "Epoch 23/30\n",
      "2805/2805 [==============================] - 81s 29ms/step - loss: 0.9783 - val_loss: 3.8939\n",
      "\n",
      "Epoch 00023: saving model to ./save_models/han_rae_ls4_v1_23_3.89393.h5\n",
      "Epoch 24/30\n",
      "2805/2805 [==============================] - 82s 29ms/step - loss: 0.9919 - val_loss: 4.1758\n",
      "\n",
      "Epoch 00024: saving model to ./save_models/han_rae_ls4_v1_24_4.17576.h5\n",
      "Epoch 25/30\n",
      "2805/2805 [==============================] - 82s 29ms/step - loss: 0.9068 - val_loss: 4.3706\n",
      "\n",
      "Epoch 00025: saving model to ./save_models/han_rae_ls4_v1_25_4.37064.h5\n",
      "Epoch 26/30\n",
      " 128/2805 [>.............................] - ETA: 1:14 - loss: 0.8119"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-85ad9ffde997>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     history = model.fit(x=[train_X_data], y=[train_Y_data], batch_size=32, epochs=30,\n\u001b[1;32m---> 39\u001b[1;33m                         verbose=True, validation_data=(val_X_data, val_Y_data), callbacks=[es, mc])\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"time :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mlc\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\mlc\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mlc\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3792\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3794\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mlc\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m     \"\"\"\n\u001b[1;32m-> 1605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mlc\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1645\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mlc\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\mlc\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\.conda\\envs\\mlc\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    # first, build a sentence encoder\n",
    "    word_input = Input(shape=(MAX_SENTENCE_LENGTH,), dtype='float32')\n",
    "    word_sequences = embedding_layer(word_input)\n",
    "    word_lstm = Bidirectional(LSTM(100, return_sequences=True, kernel_regularizer=l2_reg))(word_sequences)\n",
    "    word_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(word_lstm)\n",
    "    word_att = AttentionWithContext()(word_dense)\n",
    "    wordEncoder = Model(word_input, word_att)\n",
    "\n",
    "    # then, build a document encoder\n",
    "    sent_input = Input(shape=(MAX_SENTENCES, MAX_SENTENCE_LENGTH), dtype='float32')\n",
    "    sent_encoder = TimeDistributed(wordEncoder)(sent_input)\n",
    "    sent_lstm = Bidirectional(LSTM(100, return_sequences=True, kernel_regularizer=l2_reg))(sent_encoder)\n",
    "    sent_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(sent_lstm)\n",
    "    sent_att = AttentionWithContext()(sent_dense)\n",
    "\n",
    "    # finally, add fc layers for classification\n",
    "    hidden = BatchNormalization()(sent_att)\n",
    "    hidden = Dense(100, activation='relu')(hidden)\n",
    "    hidden = Dropout(0.2)(hidden)\n",
    "    hidden = Dense(50, activation='relu')(hidden)\n",
    "    preds = Dense(4)(hidden)\n",
    "    \n",
    "    model = Model(inputs=[sent_input], outputs=[preds])\n",
    "\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss=['mse'], optimizer=optimizer)\n",
    "\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    model_path = './save_models/han_rae_ls4_{}'.format(version) + '_{epoch:02d}_{val_loss:.5f}.h5'\n",
    "    mc = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, mode='auto')\n",
    "\n",
    "       \n",
    "    history = model.fit(x=[train_X_data], y=[train_Y_data], batch_size=32, epochs=30,\n",
    "                        verbose=True, validation_data=(val_X_data, val_Y_data), callbacks=[es, mc])\n",
    "    \n",
    "print(\"time :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 20, 200)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 20, 200)           4364000   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 20, 200)           240800    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 20, 200)           40200     \n",
      "_________________________________________________________________\n",
      "attention_with_context_2 (At (None, 200)               40400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 204       \n",
      "=================================================================\n",
      "Total params: 4,711,554\n",
      "Trainable params: 588,554\n",
      "Non-trainable params: 4,123,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-b2dcc5d67f0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-70c7678dae18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# summarize history for loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model mse'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model mse')\n",
    "plt.ylabel('mse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import Bidirectional, TimeDistributed, LSTM, Conv1D, MaxPooling1D\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "from keras.layers import Lambda, Permute, RepeatVector, Multiply\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'W_regularizer': self.W_regularizer,\n",
    "            'u_regularizer': self.u_regularizer,\n",
    "            'b_regularizer': self.b_regularizer,\n",
    "            'W_constraint': self.W_constraint,\n",
    "            'u_constraint': self.u_constraint,\n",
    "            'b_constraint': self.b_constraint,\n",
    "            'bias': self.bias\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatibl|e with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import CustomObjectScope\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAN load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with CustomObjectScope({'AttentionWithContext': AttentionWithContext}):\n",
    "    model = load_model('./save_models/best_models/han_rae_ls4_v1_13_4.03284.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "935/935 [==============================] - 4s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.115314767959921"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X_data, test_Y_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.8459873 , 2.6324384 , 2.0216837 , 1.7911365 ],\n",
       "       [2.4689155 , 1.7334155 , 3.2179945 , 2.2769032 ],\n",
       "       [3.386348  , 1.6841891 , 1.6919729 , 2.4997067 ],\n",
       "       ...,\n",
       "       [2.3717067 , 2.807764  , 1.5225168 , 2.1571414 ],\n",
       "       [2.4192858 , 3.632702  , 2.0157661 , 3.4151511 ],\n",
       "       [0.6125083 , 1.1320395 , 1.2665185 , 0.74803984]], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(test_X_data, batch_size=32)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\mlc\\lib\\site-packages\\keras\\engine\\saving.py:384: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "decoder = load_model('./save_models/decoder_models/residual_decoder_v17.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5572775e-02, 1.9839330e-04, 9.2188314e-02, ..., 2.4838612e-04,\n",
       "        7.2793016e-05, 5.6436346e-03],\n",
       "       [7.4132280e-05, 2.3853459e-06, 3.7414566e-06, ..., 5.9405076e-05,\n",
       "        9.5612251e-10, 7.4487674e-04],\n",
       "       [1.5998392e-04, 2.2189079e-07, 2.3711791e-04, ..., 2.0865756e-04,\n",
       "        1.9511448e-09, 1.2723395e-07],\n",
       "       ...,\n",
       "       [2.9906381e-02, 9.8988439e-06, 7.1566470e-02, ..., 2.5679674e-05,\n",
       "        6.3601352e-04, 7.8002733e-05],\n",
       "       [3.5010173e-03, 3.6769848e-08, 5.4273009e-01, ..., 5.3627014e-07,\n",
       "        6.3129970e-05, 3.1685006e-05],\n",
       "       [9.0681853e-05, 1.3448643e-02, 7.6588853e-03, ..., 1.1669278e-02,\n",
       "        6.9888003e-08, 1.6479942e-01]], dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_decode = decoder.predict(pred)\n",
    "test_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(test_decode[:, :], test_decode[:, :])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_predict = test_decode.round()\\ntest_predict'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"test_predict = test_decode.round()\n",
    "test_predict\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict = np.where(test_decode > 0.5, 1, 0)\n",
    "test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c_matrix = multilabel_confusion_matrix(one_hot_test_labels, predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#c_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test label load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3D프린팅</th>\n",
       "      <th>4차산업</th>\n",
       "      <th>4차산업혁명</th>\n",
       "      <th>STEAM교육</th>\n",
       "      <th>가상현실</th>\n",
       "      <th>감성</th>\n",
       "      <th>감성분석</th>\n",
       "      <th>감정</th>\n",
       "      <th>강한인공지능</th>\n",
       "      <th>강화학습</th>\n",
       "      <th>...</th>\n",
       "      <th>핀테크</th>\n",
       "      <th>학습</th>\n",
       "      <th>학습동기</th>\n",
       "      <th>학습성과</th>\n",
       "      <th>학업성취도</th>\n",
       "      <th>합성곱신경망</th>\n",
       "      <th>핵심역량</th>\n",
       "      <th>헬스케어</th>\n",
       "      <th>혁신</th>\n",
       "      <th>협업</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>935 rows × 262 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     3D프린팅  4차산업  4차산업혁명  STEAM교육  가상현실  감성  감성분석  감정  강한인공지능  강화학습  ...  핀테크  \\\n",
       "0        0     0       1        0     0   0     0   0       0     0  ...    0   \n",
       "1        0     0       0        0     0   0     0   0       0     0  ...    0   \n",
       "2        0     0       0        0     0   0     0   0       0     0  ...    0   \n",
       "3        0     0       0        0     0   0     0   0       0     0  ...    0   \n",
       "4        0     0       0        0     0   0     0   0       0     0  ...    0   \n",
       "..     ...   ...     ...      ...   ...  ..   ...  ..     ...   ...  ...  ...   \n",
       "930      0     0       0        0     0   0     0   0       0     0  ...    0   \n",
       "931      0     0       1        0     0   0     0   0       0     0  ...    0   \n",
       "932      0     0       0        0     0   0     0   0       0     0  ...    0   \n",
       "933      0     0       0        0     0   0     0   0       0     0  ...    0   \n",
       "934      0     0       0        0     0   0     0   0       0     0  ...    0   \n",
       "\n",
       "     학습  학습동기  학습성과  학업성취도  합성곱신경망  핵심역량  헬스케어  혁신  협업  \n",
       "0     0     0     0      0       0     0     0   0   0  \n",
       "1     0     0     0      0       0     0     0   0   0  \n",
       "2     0     0     0      0       0     0     0   0   0  \n",
       "3     0     0     0      0       0     0     0   0   0  \n",
       "4     0     0     0      0       0     0     0   0   0  \n",
       "..   ..   ...   ...    ...     ...   ...   ...  ..  ..  \n",
       "930   0     0     0      0       0     0     0   0   0  \n",
       "931   0     0     0      0       0     0     0   0   0  \n",
       "932   0     0     0      0       0     0     0   0   0  \n",
       "933   0     0     0      0       0     0     0   0   0  \n",
       "934   0     0     0      0       0     0     0   0   0  \n",
       "\n",
       "[935 rows x 262 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = pd.read_excel('./data/paper_test.xlsx')\n",
    "test_X = test_X.drop(['Unnamed: 0', 'abstract'], axis=1)\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(935, 262)\n"
     ]
    }
   ],
   "source": [
    "one_hot_test_labels = np.array(test_X)\n",
    "print(one_hot_test_labels)\n",
    "print(one_hot_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, f1_score, recall_score, precision_score, accuracy_score, hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.08342245989304813\n",
      "precision :  0.4301948051948052\n",
      "recall :  0.15954244431065623\n",
      "f1 :  0.23276240667545017\n",
      "------------------------\n",
      "hamming_loss :  0.007131485488018941\n"
     ]
    }
   ],
   "source": [
    "print('accuracy : ', accuracy_score(one_hot_test_labels, test_predict))\n",
    "print('precision : ', precision_score(one_hot_test_labels, test_predict, average='micro'))\n",
    "print('recall : ', recall_score(one_hot_test_labels, test_predict, average='micro'))\n",
    "print('f1 : ', f1_score(one_hot_test_labels, test_predict, average='micro'))\n",
    "print('------------------------')\n",
    "print('hamming_loss : ', hamming_loss(one_hot_test_labels, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.08342245989304813\n",
      "precision :  0.264349376114082\n",
      "recall :  0.16914438502673795\n",
      "f1 :  0.19231983702571936\n",
      "------------------------\n",
      "hamming_loss :  0.007131485488018941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\mlc\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('accuracy : ', accuracy_score(one_hot_test_labels, test_predict))\n",
    "print('precision : ', precision_score(one_hot_test_labels, test_predict, average='samples'))\n",
    "print('recall : ', recall_score(one_hot_test_labels, test_predict, average='samples'))\n",
    "print('f1 : ', f1_score(one_hot_test_labels, test_predict, average='samples'))\n",
    "print('------------------------')\n",
    "print('hamming_loss : ', hamming_loss(one_hot_test_labels, test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong example\n",
    "attention_extractor = Model(inputs=[document_input],\n",
    "                            outputs=[word_attention, sentence_attention])\n",
    "attention_extractor.predict(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_attention_extractor = Model(inputs=[sentence_input],\n",
    "                                 outputs=[word_attention])\n",
    "\n",
    "word_attentions = TimeDistributed(word_attention_extractor)(document_input)\n",
    "\n",
    "attention_extractor = Model(inputs=[document_input],\n",
    "                            outputs=[word_attentions, sentence_attention])\n",
    "attention_extractor.predict(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_rev_index = {}\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    word_rev_index[i] = word\n",
    "\n",
    "def sentiment_analysis(review):    \n",
    "    sentences = sent_tokenize(review)\n",
    "    tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "    tokenized_sentences = pad_sequences(tokenized_sentences, maxlen=MAX_SENTENCE_LENGTH)\n",
    "    pad_size = MAX_SENTENCES - tokenized_sentences.shape[0]\n",
    "\n",
    "    if pad_size <= 0:\n",
    "        tokenized_sentences = tokenized_sentences[:MAX_SENTENCES]\n",
    "    else:\n",
    "        tokenized_sentences = np.pad(\n",
    "            tokenized_sentences, ((0, pad_size), (0, 0)),\n",
    "            mode='constant', constant_values=0\n",
    "        )\n",
    "    \n",
    "    # word attention만 가져오기\n",
    "    pred_attention = attention_extractor.predict(np.asarray([tokenized_sentences]))[0]\n",
    "    for i, sentence in enumerate(tokenized_sentences[:-pad_size]):\n",
    "        words = [word_rev_index[word_id] for word_id in sentence if word_id != 0][:50]\n",
    "        pred_att = np.asarray(pred_attention[0][i][::-1][:len(words)][::-1])\n",
    "        pred_att = np.expand_dims(pred_att, axis=0)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(len(words), 2))\n",
    "        plt.rc('xtick', labelsize=22)\n",
    "        heatmap = sn.heatmap(pred_att, xticklabels=words, square=True, linewidths=0.1)\n",
    "        plt.xticks(rotation=70)\n",
    "        plt.show()\n",
    "        \n",
    "sentiment_analysis(\"Delicious healthy food. The steak is amazing. Fish and pork are awesome too. Service is above and beyond. Not a bad thing to say about this place. Worth every penny!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
