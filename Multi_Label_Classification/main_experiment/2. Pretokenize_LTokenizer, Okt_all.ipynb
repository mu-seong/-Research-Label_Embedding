{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_excel('./paper_labels_3_all_random_300_train.xlsx')\n",
    "val = pd.read_excel('./paper_labels_3_all_random_300_val.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abst = train['abstract']\n",
    "val_abst = val['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_abst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent = list(train_abst)\n",
    "train_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sent = list(val_abst)\n",
    "val_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_sent + val_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_extractor = WordExtractor(min_frequency = 1, min_cohesion_forward = 0.0, min_right_branching_entropy = 0.0)\n",
    "\n",
    "word_extractor.train(sentences)\n",
    "word_extractor.save('./wordextractor_model_all_random_300')\n",
    "\n",
    "words = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = word_extractor.word_scores()\n",
    "cal_scores = {key:(scores[key].cohesion_forward * math.exp(scores[key].right_branching_entropy)) for key in scores.keys()}\n",
    "tokenizer = LTokenizer(scores = cal_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltokenized_data = []\n",
    "\n",
    "\n",
    "f = open('./user_dic_all_random_300.txt', 'w', encoding='UTF8')\n",
    "for sentence in tqdm(sentences):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        token = token+'\\n'\n",
    "        f.write(token)\n",
    "        ltokenized_data.append(token)\n",
    "        print(token)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ltokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "tokenized_data = []\n",
    "\n",
    "\n",
    "for sentence in tqdm(sentences):\n",
    "    temp_X = okt.morphs(sentence)\n",
    "    tokenized_data.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# okt 사용자 사전 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/user/.conda/envs/mlc/lib/site-packages/konlpy/java')\n",
    "os.getcwd() \n",
    "os.makedirs('./userdic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jar xvf open-korean-text-2.1.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jar cvf open-korean-text-2.1.0.jar *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "os.chdir('D:/Anaconda3/envs/han/lib/site-packages/konlpy/java')\n",
    "os.getcwd() \n",
    "os.makedirs('./userdic')\n",
    "\n",
    "\n",
    "!jar xvf ../open-korean-text-2.1.0.jar\n",
    "\n",
    "\n",
    "with open(f\"D:/Anaconda3/envs/han/lib/site-packages/konlpy/java/userdic/org/openkoreantext/processor/util/noun/names.txt\", encoding='UTF8') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "    \n",
    "for tokens in tqdm(ltokenized_data):\n",
    "    for token in tokens:\n",
    "        data += token + '\\n'\n",
    "        \n",
    "        \n",
    "with open(\"D:/Anaconda3/envs/han/lib/site-packages/konlpy/java/userdic/org/openkoreantext/processor/util/noun/names.txt\", 'w', encoding='UTF8') as f:\n",
    "    f.write(data)\n",
    "    \n",
    "    \n",
    "!jar cvf ../open-korean-text-2.1.0.jar * \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 명사 추출 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "noun_extractor = LRNounExtractor_v2()\n",
    "nouns = noun_extractor.train_extract(sentences) # list of str like\n",
    "\n",
    "noun_scores = {noun:score.score for noun, score in nouns.items()}\n",
    "cohesion_score = {word:score.cohesion_forward for word, score in words.items()}\n",
    "combined_scores = {noun:score + cohesion_score.get(noun, 0) for noun, score in noun_scores.items()}\n",
    "combined_scores.update({subword:cohesion for subword, cohesion in cohesion_score.items() if not (subword in combined_scores)})\n",
    "\n",
    "tokenizer = LTokenizer(scores=combined_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "noun_tokenized_data = []\n",
    "\n",
    "\n",
    "for sentence in tqdm(sentences):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    noun_tokenized_data.append(tokens)\n",
    "    print(tokens)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['본', '논문에서는', '최신', '의', '기계', '학습', '모델을', '사용하여', '서울', '아파트', '거래', '가격에', '대한', '자동', '평가', '모델을', '개발', '하였다', '.']\n",
      "['지능', '정보사회의', '도래', '로', '종래와는', '차원', '이', '다른', '새로운', '이슈들', '이', '제기', '될', '것으로', '전망', '된다', '.']\n",
      "['최근', '빅데이터', '분석과', '인공지능을', '활용한', '정보', '서비스로', '이용자의', '편리성', '을', '확대', '하고', ',', '미래', '의', '상황을', '예측', '하는', '등', '새로운', '가치', '창출', '시스템으로', '지능', '형서비스가', '등장', '하였다', '.']\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "\n",
    "#rae_v17_train_10\n",
    "sentence1 = \"본 논문에서는 최신의 기계 학습 모델을 사용하여 서울 아파트 거래 가격에 대한 자동 평가 모델을 개발하였다.\"\n",
    "\n",
    "#rae_v17_train_5\n",
    "sentence2 = \"지능정보사회의 도래로 종래와는 차원이 다른 새로운 이슈들이 제기될 것으로 전망된다.\"\n",
    "\n",
    "#rae_v17_train_7\n",
    "sentence3 = \"최근 빅데이터 분석과 인공지능을 활용한 정보서비스로 이용자의 편리성을 확대하고, 미래의 상황을 예측하는 등 새로운 가치 창출 시스템으로 지능형서비스가 등장하였다.\"\n",
    "\n",
    "print(okt.morphs(sentence1))\n",
    "print(okt.morphs(sentence2))\n",
    "print(okt.morphs(sentence3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
